{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import checklist\n",
    "from checklist.test_suite import TestSuite\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline(\"question-answering\", model=\"./model-qa-trained/\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_path = '../../../checklist/release_data/squad/squad_suite.pkl'\n",
    "suite = TestSuite.from_file(suite_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predconfs(context_question_pairs):\n",
    "    preds = []\n",
    "    confs = []\n",
    "    for c, q in context_question_pairs:\n",
    "        try:\n",
    "            p = model(question=q, context=c, truncation=True, )\n",
    "        except:\n",
    "            print('Failed', q)\n",
    "            preds.append(' ')\n",
    "            confs.append(1)\n",
    "        preds.append(p['answer'])\n",
    "        confs.append(p['score'])\n",
    "    return preds, np.array(confs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running A is COMP than B. Who is more / less COMP?\n",
      "Predicting 200 examples\n",
      "Running Intensifiers (very, super, extremely) and reducers (somewhat, kinda, etc)?\n",
      "Predicting 1200 examples\n",
      "Running size, shape, age, color\n",
      "Predicting 400 examples\n",
      "Running Profession vs nationality\n",
      "Predicting 1000 examples\n",
      "Running Animal vs Vehicle\n",
      "Predicting 400 examples\n",
      "Running Animal vs Vehicle v2\n",
      "Predicting 400 examples\n",
      "Running Synonyms\n",
      "Predicting 400 examples\n",
      "Running A is COMP than B. Who is antonym(COMP)? B\n",
      "Predicting 400 examples\n",
      "Running A is more X than B. Who is more antonym(X)? B. Who is less X? B. Who is more X? A. Who is less antonym(X)? A.\n",
      "Predicting 1600 examples\n",
      "Running Question typo\n",
      "Predicting 200 examples\n",
      "Running Question contractions\n",
      "Predicting 201 examples\n",
      "Running Add random sentence to context\n",
      "Predicting 300 examples\n",
      "Running Change name everywhere\n",
      "Predicting 1100 examples\n",
      "Running Change location everywhere\n",
      "Predicting 1100 examples\n",
      "Running There was a change in profession\n",
      "Predicting 200 examples\n",
      "Running Understanding before / after -> first / last.\n",
      "Predicting 400 examples\n",
      "Running Negation in context, may or may not be in question\n",
      "Predicting 400 examples\n",
      "Running Negation in question only.\n",
      "Predicting 800 examples\n",
      "Running M/F failure rates should be similar for different professions\n",
      "Predicting 100 examples\n",
      "Running Basic coref, he / she\n",
      "Predicting 800 examples\n",
      "Running Basic coref, his / her\n",
      "Predicting 200 examples\n",
      "Running Former / Latter\n",
      "Predicting 400 examples\n",
      "Running Agent / object distinction\n",
      "Predicting 400 examples\n",
      "Running Agent / object distinction with 3 agents\n",
      "Predicting 1600 examples\n"
     ]
    }
   ],
   "source": [
    "suite.run(predconfs, overwrite=True, n=100)   # for quicker testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_squad_with_context(x, pred, conf, label=None, *args, **kwargs):\n",
    "    c, q = x\n",
    "    ret = 'C: %s\\nQ: %s\\n' % (c, q)\n",
    "    if label is not None:\n",
    "        ret += 'A: %s\\n' % label\n",
    "    ret += 'P: %s\\n' % pred\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary\n",
      "\n",
      "A is COMP than B. Who is more / less COMP?\n",
      "Test cases:      494\n",
      "Test cases run:  100\n",
      "Fails (rate):    99 (99.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Taylor is greater than Alexis.\n",
      "Q: Who is less great?\n",
      "A: Alexis\n",
      "P: Taylor\n",
      "\n",
      "\n",
      "----\n",
      "C: Kimberly is taller than Steven.\n",
      "Q: Who is less tall?\n",
      "A: Steven\n",
      "P: Kimberly\n",
      "\n",
      "\n",
      "----\n",
      "C: Amber is cleaner than Abigail.\n",
      "Q: Who is less clean?\n",
      "A: Abigail\n",
      "P: Amber\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Intensifiers (very, super, extremely) and reducers (somewhat, kinda, etc)?\n",
      "Test cases:      497\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Dylan is slightly particular about the project. Patrick is particular about the project.\n",
      "Q: Who is least particular about the project?\n",
      "A: Dylan\n",
      "P: Patrick\n",
      "\n",
      "C: Patrick is super particular about the project. Dylan is particular about the project.\n",
      "Q: Who is most particular about the project?\n",
      "A: Patrick\n",
      "P: Dylan\n",
      "\n",
      "C: Patrick is particular about the project. Dylan is slightly particular about the project.\n",
      "Q: Who is most particular about the project?\n",
      "A: Patrick\n",
      "P: Dylan\n",
      "\n",
      "\n",
      "----\n",
      "C: Michael is incredibly passionate about the project. Tiffany is passionate about the project.\n",
      "Q: Who is most passionate about the project?\n",
      "A: Michael\n",
      "P: Tiffany\n",
      "\n",
      "C: Tiffany is passionate about the project. Michael is incredibly passionate about the project.\n",
      "Q: Who is most passionate about the project?\n",
      "A: Michael\n",
      "P: Tiffany\n",
      "\n",
      "C: Tiffany is slightly passionate about the project. Michael is passionate about the project.\n",
      "Q: Who is most passionate about the project?\n",
      "A: Michael\n",
      "P: Tiffany\n",
      "\n",
      "\n",
      "----\n",
      "C: Dylan is pleased about the project. Kelly is slightly pleased about the project.\n",
      "Q: Who is least pleased about the project?\n",
      "A: Kelly\n",
      "P: Dylan is pleased about the project. Kelly\n",
      "\n",
      "C: Dylan is pleased about the project. Kelly is slightly pleased about the project.\n",
      "Q: Who is most pleased about the project?\n",
      "A: Dylan\n",
      "P: Dylan is pleased about the project. Kelly\n",
      "\n",
      "C: Kelly is slightly pleased about the project. Dylan is pleased about the project.\n",
      "Q: Who is least pleased about the project?\n",
      "A: Kelly\n",
      "P: Dylan\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taxonomy\n",
      "\n",
      "size, shape, age, color\n",
      "Test cases:      500\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: There is a thing in the room. The thing is small and green.\n",
      "Q: What size is the thing?\n",
      "A: small\n",
      "P: small and green\n",
      "\n",
      "C: There is a thing in the room. The thing is small and green.\n",
      "Q: What color is the thing?\n",
      "A: green\n",
      "P: small and green\n",
      "\n",
      "C: There is a small green thing in the room.\n",
      "Q: What size is the thing?\n",
      "A: small\n",
      "P: green\n",
      "\n",
      "\n",
      "----\n",
      "C: There is a small purple table in the room.\n",
      "Q: What size is the table?\n",
      "A: small\n",
      "P: purple\n",
      "\n",
      "C: There is a table in the room. The table is small and purple.\n",
      "Q: What size is the table?\n",
      "A: small\n",
      "P: small and purple\n",
      "\n",
      "\n",
      "----\n",
      "C: There is a table in the room. The table is old and red.\n",
      "Q: What age is the table?\n",
      "A: old\n",
      "P: old and red\n",
      "\n",
      "C: There is an old red table in the room.\n",
      "Q: What age is the table?\n",
      "A: old\n",
      "P: red\n",
      "\n",
      "C: There is a table in the room. The table is old and red.\n",
      "Q: What color is the table?\n",
      "A: red\n",
      "P: old and red\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Profession vs nationality\n",
      "Test cases:      500\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Joseph is an American investigator.\n",
      "Q: What is Joseph's job?\n",
      "A: investigator\n",
      "P: American investigator\n",
      "\n",
      "C: Joseph is an American investigator.\n",
      "Q: What is Joseph's nationality?\n",
      "A: American\n",
      "P: American investigator\n",
      "\n",
      "C: Joseph is an investigator. Joseph is American.\n",
      "Q: What is Joseph's job?\n",
      "A: investigator\n",
      "P: American\n",
      "\n",
      "\n",
      "----\n",
      "C: Mark is a Bangladeshi historian.\n",
      "Q: What is Mark's job?\n",
      "A: historian\n",
      "P: Bangladeshi historian\n",
      "\n",
      "C: Mark is a historian. Mark is Bangladeshi.\n",
      "Q: What is Mark's job?\n",
      "A: historian\n",
      "P: Bangladeshi\n",
      "\n",
      "C: Mark is Bangladeshi. Mark is a historian.\n",
      "Q: What is Mark's job?\n",
      "A: historian\n",
      "P: Bangladeshi. Mark is a historian\n",
      "\n",
      "\n",
      "----\n",
      "C: Daniel is a Bangladeshi historian.\n",
      "Q: What is Daniel's job?\n",
      "A: historian\n",
      "P: Bangladeshi historian\n",
      "\n",
      "C: Daniel is a historian. Daniel is Bangladeshi.\n",
      "Q: What is Daniel's job?\n",
      "A: historian\n",
      "P: Bangladeshi\n",
      "\n",
      "C: Daniel is Bangladeshi. Daniel is a historian.\n",
      "Q: What is Daniel's job?\n",
      "A: historian\n",
      "P: Bangladeshi. Daniel is a historian\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Animal vs Vehicle\n",
      "Test cases:      500\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Jeffrey has a dog and a car.\n",
      "Q: What animal does Jeffrey have?\n",
      "A: dog\n",
      "P: dog and a car\n",
      "\n",
      "C: Jeffrey has a dog and a car.\n",
      "Q: What vehicle does Jeffrey have?\n",
      "A: car\n",
      "P: dog and a car\n",
      "\n",
      "C: Jeffrey has a car and a dog.\n",
      "Q: What vehicle does Jeffrey have?\n",
      "A: car\n",
      "P: car and a dog\n",
      "\n",
      "\n",
      "----\n",
      "C: David has a duck and a truck.\n",
      "Q: What animal does David have?\n",
      "A: duck\n",
      "P: duck and a truck\n",
      "\n",
      "C: David has a duck and a truck.\n",
      "Q: What vehicle does David have?\n",
      "A: truck\n",
      "P: duck and a truck\n",
      "\n",
      "C: David has a truck and a duck.\n",
      "Q: What vehicle does David have?\n",
      "A: truck\n",
      "P: truck and a duck\n",
      "\n",
      "\n",
      "----\n",
      "C: Erin has a serpent and a train.\n",
      "Q: What animal does Erin have?\n",
      "A: serpent\n",
      "P: serpent and a train\n",
      "\n",
      "C: Erin has a serpent and a train.\n",
      "Q: What vehicle does Erin have?\n",
      "A: train\n",
      "P: serpent and a train\n",
      "\n",
      "C: Erin has a train and a serpent.\n",
      "Q: What vehicle does Erin have?\n",
      "A: train\n",
      "P: train and a serpent\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Animal vs Vehicle v2\n",
      "Test cases:      496\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Robert bought a hamster. Jeremy bought a van.\n",
      "Q: Who bought an animal?\n",
      "A: Robert\n",
      "P: van\n",
      "\n",
      "C: Robert bought a hamster. Jeremy bought a van.\n",
      "Q: Who bought a vehicle?\n",
      "A: Jeremy\n",
      "P: van\n",
      "\n",
      "C: Jeremy bought a van. Robert bought a hamster.\n",
      "Q: Who bought an animal?\n",
      "A: Robert\n",
      "P: Robert bought a hamster\n",
      "\n",
      "\n",
      "----\n",
      "C: Victoria bought a serpent. Lauren bought a bike.\n",
      "Q: Who bought an animal?\n",
      "A: Victoria\n",
      "P: serpent. Lauren bought a bike\n",
      "\n",
      "C: Victoria bought a serpent. Lauren bought a bike.\n",
      "Q: Who bought a vehicle?\n",
      "A: Lauren\n",
      "P: serpent. Lauren bought a bike\n",
      "\n",
      "C: Lauren bought a bike. Victoria bought a serpent.\n",
      "Q: Who bought an animal?\n",
      "A: Victoria\n",
      "P: serpent\n",
      "\n",
      "\n",
      "----\n",
      "C: Noah bought a bull. Steven bought a truck.\n",
      "Q: Who bought an animal?\n",
      "A: Noah\n",
      "P: Steven\n",
      "\n",
      "C: Steven bought a truck. Noah bought a bull.\n",
      "Q: Who bought an animal?\n",
      "A: Noah\n",
      "P: Steven\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Synonyms\n",
      "Test cases:      447\n",
      "Test cases run:  100\n",
      "Fails (rate):    94 (94.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Jason is very grateful. Lisa is very happy.\n",
      "Q: Who is thankful?\n",
      "A: Jason\n",
      "P: Lisa is very happy\n",
      "\n",
      "C: Lisa is very happy. Jason is very grateful.\n",
      "Q: Who is joyful?\n",
      "A: Lisa\n",
      "P: Jason is very grateful\n",
      "\n",
      "\n",
      "----\n",
      "C: Katherine is very intelligent. Kimberly is very happy.\n",
      "Q: Who is smart?\n",
      "A: Katherine\n",
      "P: Kimberly is very happy\n",
      "\n",
      "C: Katherine is very intelligent. Kimberly is very happy.\n",
      "Q: Who is joyful?\n",
      "A: Kimberly\n",
      "P: Kimberly is very happy\n",
      "\n",
      "C: Kimberly is very happy. Katherine is very intelligent.\n",
      "Q: Who is smart?\n",
      "A: Katherine\n",
      "P: Katherine is very intelligent\n",
      "\n",
      "\n",
      "----\n",
      "C: Tyler is very intelligent. Victoria is very vocal.\n",
      "Q: Who is smart?\n",
      "A: Tyler\n",
      "P: Victoria is very vocal\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "A is COMP than B. Who is antonym(COMP)? B\n",
      "Test cases:      496\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Amy is warmer than Mark.\n",
      "Q: Who is colder?\n",
      "A: Mark\n",
      "P: Amy\n",
      "\n",
      "C: Mark is colder than Amy.\n",
      "Q: Who is colder?\n",
      "A: Mark\n",
      "P: Amy\n",
      "\n",
      "\n",
      "----\n",
      "C: Sean is warmer than Kyle.\n",
      "Q: Who is colder?\n",
      "A: Kyle\n",
      "P: Sean\n",
      "\n",
      "C: Kyle is colder than Sean.\n",
      "Q: Who is colder?\n",
      "A: Kyle\n",
      "P: Kyle is colder than Sean\n",
      "\n",
      "C: Kyle is colder than Sean.\n",
      "Q: Who is warmer?\n",
      "A: Sean\n",
      "P: Kyle is colder than Sean.\n",
      "\n",
      "\n",
      "----\n",
      "C: Amy is lighter than John.\n",
      "Q: Who is darker?\n",
      "A: John\n",
      "P: Amy\n",
      "\n",
      "C: John is darker than Amy.\n",
      "Q: Who is darker?\n",
      "A: John\n",
      "P: Amy\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "A is more X than B. Who is more antonym(X)? B. Who is less X? B. Who is more X? A. Who is less antonym(X)? A.\n",
      "Test cases:      491\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Samuel is more positive than Jessica.\n",
      "Q: Who is more negative?\n",
      "A: Jessica\n",
      "P: Samuel\n",
      "\n",
      "C: Samuel is less negative than Jessica.\n",
      "Q: Who is more negative?\n",
      "A: Jessica\n",
      "P: Samuel\n",
      "\n",
      "C: Samuel is more positive than Jessica.\n",
      "Q: Who is less positive?\n",
      "A: Jessica\n",
      "P: Samuel\n",
      "\n",
      "\n",
      "----\n",
      "C: Katherine is more courageous than Natalie.\n",
      "Q: Who is less courageous?\n",
      "A: Natalie\n",
      "P: Katherine\n",
      "\n",
      "C: Katherine is less fearful than Natalie.\n",
      "Q: Who is more fearful?\n",
      "A: Natalie\n",
      "P: Katherine\n",
      "\n",
      "C: Natalie is less courageous than Katherine.\n",
      "Q: Who is more courageous?\n",
      "A: Katherine\n",
      "P: Natalie\n",
      "\n",
      "\n",
      "----\n",
      "C: Amy is more secure than Lauren.\n",
      "Q: Who is less secure?\n",
      "A: Lauren\n",
      "P: Amy\n",
      "\n",
      "C: Amy is more secure than Lauren.\n",
      "Q: Who is more insecure?\n",
      "A: Lauren\n",
      "P: Amy\n",
      "\n",
      "C: Amy is less insecure than Lauren.\n",
      "Q: Who is less secure?\n",
      "A: Lauren\n",
      "P: Amy\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Robustness\n",
      "\n",
      "Question typo\n",
      "Test cases:      500\n",
      "Test cases run:  100\n",
      "Fails (rate):    20 (20.0%)\n",
      "\n",
      "Example fails:\n",
      "C: The plague theory was first significantly challenged by the work of British bacteriologist J. F. D. Shrewsbury in 1970, who noted that the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague, leading him to conclude that contemporary accounts were exaggerations. In 1984 zoologist Graham Twigg produced the first major work to challenge the bubonic plague theory directly, and his doubts about the identity of the Black Death have been taken up by a number of authors, including Samuel K. Cohn, Jr. (2002), David Herlihy (1997), and Susan Scott and Christopher Duncan (2001).\n",
      "Q: Who discussed Twigg's study in 2002?\n",
      "P: Susan Scott and Christopher Duncan\n",
      "\n",
      "C: The plague theory was first significantly challenged by the work of British bacteriologist J. F. D. Shrewsbury in 1970, who noted that the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague, leading him to conclude that contemporary accounts were exaggerations. In 1984 zoologist Graham Twigg produced the first major work to challenge the bubonic plague theory directly, and his doubts about the identity of the Black Death have been taken up by a number of authors, including Samuel K. Cohn, Jr. (2002), David Herlihy (1997), and Susan Scott and Christopher Duncan (2001).\n",
      "Q: Who dicsussed Twigg's study in 2002?\n",
      "P: David Herlihy (1997), and Susan Scott and Christopher Duncan\n",
      "\n",
      "\n",
      "----\n",
      "C: In 2013, the Peabody Awards honoured Doctor Who with an Institutional Peabody \"for evolving with technology and the times like nothing else in the known television universe.\" The programme is listed in Guinness World Records as the longest-running science fiction television show in the world, the \"most successful\" science fiction series of all time—based on its over-all broadcast ratings, DVD and book sales, and iTunes traffic— and for the largest ever simulcast of a TV drama with its 50th anniversary special. During its original run, it was recognised for its imaginative stories, creative low-budget special effects, and pioneering use of electronic music (originally produced by the BBC Radiophonic Workshop).\n",
      "Q: What year did Doctor Who win a Peabody award?\n",
      "P: 2013\n",
      "\n",
      "C: In 2013, the Peabody Awards honoured Doctor Who with an Institutional Peabody \"for evolving with technology and the times like nothing else in the known television universe.\" The programme is listed in Guinness World Records as the longest-running science fiction television show in the world, the \"most successful\" science fiction series of all time—based on its over-all broadcast ratings, DVD and book sales, and iTunes traffic— and for the largest ever simulcast of a TV drama with its 50th anniversary special. During its original run, it was recognised for its imaginative stories, creative low-budget special effects, and pioneering use of electronic music (originally produced by the BBC Radiophonic Workshop).\n",
      "Q: What yea rdid Doctor Who win a Peabody award?\n",
      "P: Institutional Peabody\n",
      "\n",
      "\n",
      "----\n",
      "C: DECnet is a suite of network protocols created by Digital Equipment Corporation, originally released in 1975 in order to connect two PDP-11 minicomputers. It evolved into one of the first peer-to-peer network architectures, thus transforming DEC into a networking powerhouse in the 1980s. Initially built with three layers, it later (1982) evolved into a seven-layer OSI-compliant networking protocol. The DECnet protocols were designed entirely by Digital Equipment Corporation. However, DECnet Phase II (and later) were open standards with published specifications, and several implementations were developed outside DEC, including one for Linux.\n",
      "Q: What is  DECnet\n",
      "P: Phase II\n",
      "\n",
      "C: DECnet is a suite of network protocols created by Digital Equipment Corporation, originally released in 1975 in order to connect two PDP-11 minicomputers. It evolved into one of the first peer-to-peer network architectures, thus transforming DEC into a networking powerhouse in the 1980s. Initially built with three layers, it later (1982) evolved into a seven-layer OSI-compliant networking protocol. The DECnet protocols were designed entirely by Digital Equipment Corporation. However, DECnet Phase II (and later) were open standards with published specifications, and several implementations were developed outside DEC, including one for Linux.\n",
      "Q: What is D ECnet\n",
      "P: Digital Equipment Corporation\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Question contractions\n",
      "Test cases:      500\n",
      "Test cases run:  100\n",
      "Fails (rate):    13 (13.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Research shows that student motivation and attitudes towards school are closely linked to student-teacher relationships. Enthusiastic teachers are particularly good at creating beneficial relations with their students. Their ability to create effective learning environments that foster student achievement depends on the kind of relationship they build with their students. Useful teacher-to-student interactions are crucial in linking academic success with personal achievement. Here, personal success is a student's internal goal of improving himself, whereas academic success includes the goals he receives from his superior. A teacher must guide his student in aligning his personal goals with his academic goals. Students who receive this positive influence show stronger self-confidence and greater personal and academic success than those without these teacher interactions.\n",
      "Q: What is strongly linked to good student-teacher relationships?\n",
      "P: student motivation and attitudes\n",
      "\n",
      "C: Research shows that student motivation and attitudes towards school are closely linked to student-teacher relationships. Enthusiastic teachers are particularly good at creating beneficial relations with their students. Their ability to create effective learning environments that foster student achievement depends on the kind of relationship they build with their students. Useful teacher-to-student interactions are crucial in linking academic success with personal achievement. Here, personal success is a student's internal goal of improving himself, whereas academic success includes the goals he receives from his superior. A teacher must guide his student in aligning his personal goals with his academic goals. Students who receive this positive influence show stronger self-confidence and greater personal and academic success than those without these teacher interactions.\n",
      "Q: What's strongly linked to good student-teacher relationships?\n",
      "P: Enthusiastic\n",
      "\n",
      "\n",
      "----\n",
      "C: Fresno (/ˈfrɛznoʊ/ FREZ-noh), the county seat of Fresno County, is a city in the U.S. state of California. As of 2015, the city's population was 520,159, making it the fifth-largest city in California, the largest inland city in California and the 34th-largest in the nation. Fresno is in the center of the San Joaquin Valley and is the largest city in the Central Valley, which contains the San Joaquin Valley. It is approximately 220 miles (350 km) northwest of Los Angeles, 170 miles (270 km) south of the state capital, Sacramento, or 185 miles (300 km) south of San Francisco. The name Fresno means \"ash tree\" in Spanish, and an ash leaf is featured on the city's flag.\n",
      "Q: What is featured on the city of Fresno's city flag?\n",
      "P: an ash leaf\n",
      "\n",
      "C: Fresno (/ˈfrɛznoʊ/ FREZ-noh), the county seat of Fresno County, is a city in the U.S. state of California. As of 2015, the city's population was 520,159, making it the fifth-largest city in California, the largest inland city in California and the 34th-largest in the nation. Fresno is in the center of the San Joaquin Valley and is the largest city in the Central Valley, which contains the San Joaquin Valley. It is approximately 220 miles (350 km) northwest of Los Angeles, 170 miles (270 km) south of the state capital, Sacramento, or 185 miles (300 km) south of San Francisco. The name Fresno means \"ash tree\" in Spanish, and an ash leaf is featured on the city's flag.\n",
      "Q: What's featured on the city of Fresno's city flag?\n",
      "P: ash leaf\n",
      "\n",
      "\n",
      "----\n",
      "C: Evolution of the adaptive immune system occurred in an ancestor of the jawed vertebrates. Many of the classical molecules of the adaptive immune system (e.g., immunoglobulins and T cell receptors) exist only in jawed vertebrates. However, a distinct lymphocyte-derived molecule has been discovered in primitive jawless vertebrates, such as the lamprey and hagfish. These animals possess a large array of molecules called Variable lymphocyte receptors (VLRs) that, like the antigen receptors of jawed vertebrates, are produced from only a small number (one or two) of genes. These molecules are believed to bind pathogenic antigens in a similar way to antibodies, and with the same degree of specificity.\n",
      "Q: What are two examples of primitive jawless vertebrates?\n",
      "P: the lamprey and hagfish\n",
      "\n",
      "C: Evolution of the adaptive immune system occurred in an ancestor of the jawed vertebrates. Many of the classical molecules of the adaptive immune system (e.g., immunoglobulins and T cell receptors) exist only in jawed vertebrates. However, a distinct lymphocyte-derived molecule has been discovered in primitive jawless vertebrates, such as the lamprey and hagfish. These animals possess a large array of molecules called Variable lymphocyte receptors (VLRs) that, like the antigen receptors of jawed vertebrates, are produced from only a small number (one or two) of genes. These molecules are believed to bind pathogenic antigens in a similar way to antibodies, and with the same degree of specificity.\n",
      "Q: What're two examples of primitive jawless vertebrates?\n",
      "P: lamprey and hagfish\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Add random sentence to context\n",
      "Test cases:      500\n",
      "Test cases run:  100\n",
      "Fails (rate):    23 (23.0%)\n",
      "\n",
      "Example fails:\n",
      "C: The 2007 Lisbon Treaty explicitly recognised fundamental rights by providing in Article 6(1) that \"The Union recognises the rights, freedoms and principles set out in the Charter of Fundamental Rights of the European Union of 7 December 2000, as adopted at Strasbourg on 12 December 2007, which shall have the same legal value as the Treaties.\" Therefore, the Charter of Fundamental Rights of the European Union has become an integral part of European Union law, codifying the fundamental rights which were previously considered general principles of European Union law. In effect, after the Lisbon Treaty, the Charter and the Convention now co-exist under European Union law, though the former is enforced by the European Court of Justice in relation to European Union measures, and the latter by the European Court of Human Rights in relation to measures by member states.\n",
      "Q: When was the Lisbon Treaty established?\n",
      "P: 12 December 2007\n",
      "\n",
      "C: The second scale compresses the most recent era, so the most recent era is expanded in the third scale. The 2007 Lisbon Treaty explicitly recognised fundamental rights by providing in Article 6(1) that \"The Union recognises the rights, freedoms and principles set out in the Charter of Fundamental Rights of the European Union of 7 December 2000, as adopted at Strasbourg on 12 December 2007, which shall have the same legal value as the Treaties.\" Therefore, the Charter of Fundamental Rights of the European Union has become an integral part of European Union law, codifying the fundamental rights which were previously considered general principles of European Union law. In effect, after the Lisbon Treaty, the Charter and the Convention now co-exist under European Union law, though the former is enforced by the European Court of Justice in relation to European Union measures, and the latter by the European Court of Human Rights in relation to measures by member states.\n",
      "Q: When was the Lisbon Treaty established?\n",
      "P: 2007\n",
      "\n",
      "\n",
      "----\n",
      "C: Following the Cretaceous–Paleogene extinction event, the extinction of the dinosaurs and the wetter climate may have allowed the tropical rainforest to spread out across the continent. From 66–34 Mya, the rainforest extended as far south as 45°. Climate fluctuations during the last 34 million years have allowed savanna regions to expand into the tropics. During the Oligocene, for example, the rainforest spanned a relatively narrow band. It expanded again during the Middle Miocene, then retracted to a mostly inland formation at the last glacial maximum. However, the rainforest still managed to thrive during these glacial periods, allowing for the survival and evolution of a broad diversity of species.\n",
      "Q: Beginning how many years ago did the amazon rainforest extend 45 degrees south?\n",
      "P: 66–34 Mya, the rainforest extended as far south as 45°\n",
      "\n",
      "C: ABC also carries various X Games weekend events not broadcast by ESPN. Following the Cretaceous–Paleogene extinction event, the extinction of the dinosaurs and the wetter climate may have allowed the tropical rainforest to spread out across the continent. From 66–34 Mya, the rainforest extended as far south as 45°. Climate fluctuations during the last 34 million years have allowed savanna regions to expand into the tropics. During the Oligocene, for example, the rainforest spanned a relatively narrow band. It expanded again during the Middle Miocene, then retracted to a mostly inland formation at the last glacial maximum. However, the rainforest still managed to thrive during these glacial periods, allowing for the survival and evolution of a broad diversity of species.\n",
      "Q: Beginning how many years ago did the amazon rainforest extend 45 degrees south?\n",
      "P: 66–34 Mya\n",
      "\n",
      "C: Following the Cretaceous–Paleogene extinction event, the extinction of the dinosaurs and the wetter climate may have allowed the tropical rainforest to spread out across the continent. From 66–34 Mya, the rainforest extended as far south as 45°. Climate fluctuations during the last 34 million years have allowed savanna regions to expand into the tropics. During the Oligocene, for example, the rainforest spanned a relatively narrow band. It expanded again during the Middle Miocene, then retracted to a mostly inland formation at the last glacial maximum. However, the rainforest still managed to thrive during these glacial periods, allowing for the survival and evolution of a broad diversity of species.ABC also carries various X Games weekend events not broadcast by ESPN. \n",
      "Q: Beginning how many years ago did the amazon rainforest extend 45 degrees south?\n",
      "P: 66–34 Mya\n",
      "\n",
      "\n",
      "----\n",
      "C: After the revocation of the Edict of Nantes, the Dutch Republic received the largest group of Huguenot refugees, an estimated total of 75,000 to 100,000 people. Amongst them were 200 clergy. Many came from the region of the Cévennes, for instance, the village of Fraissinet-de-Lozère. This was a huge influx as the entire population of the Dutch Republic amounted to ca. 2 million at that time. Around 1700, it is estimated that nearly 25% of the Amsterdam population was Huguenot.[citation needed] In 1705, Amsterdam and the area of West Frisia were the first areas to provide full citizens rights to Huguenot immigrants, followed by the Dutch Republic in 1715. Huguenots intermarried with Dutch from the outset.\n",
      "Q: What two areas in the Republic were first to grant rights to the Huguenots?\n",
      "P: Amsterdam and the area of West Frisia\n",
      "\n",
      "C: It is the third most populated megalopolis in the United States, after the Great Lakes Megalopolis and the Northeastern megalopolis. After the revocation of the Edict of Nantes, the Dutch Republic received the largest group of Huguenot refugees, an estimated total of 75,000 to 100,000 people. Amongst them were 200 clergy. Many came from the region of the Cévennes, for instance, the village of Fraissinet-de-Lozère. This was a huge influx as the entire population of the Dutch Republic amounted to ca. 2 million at that time. Around 1700, it is estimated that nearly 25% of the Amsterdam population was Huguenot.[citation needed] In 1705, Amsterdam and the area of West Frisia were the first areas to provide full citizens rights to Huguenot immigrants, followed by the Dutch Republic in 1715. Huguenots intermarried with Dutch from the outset.\n",
      "Q: What two areas in the Republic were first to grant rights to the Huguenots?\n",
      "P: West Frisia\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NER\n",
      "\n",
      "Change name everywhere\n",
      "Test cases:      500\n",
      "Test cases run:  100\n",
      "Fails (rate):    16 (16.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Jacksonville has suffered less damage from hurricanes than most other east coast cities, although the threat does exist for a direct hit by a major hurricane. The city has only received one direct hit from a hurricane since 1871; however, Jacksonville has experienced hurricane or near-hurricane conditions more than a dozen times due to storms crossing the state from the Gulf of Mexico to the Atlantic Ocean, or passing to the north or south in the Atlantic and brushing past the area. The strongest effect on Jacksonville was from Hurricane Dora in 1964, the only recorded storm to hit the First Coast with sustained hurricane-force winds. The eye crossed St. Augustine with winds that had just barely diminished to 110 mph (180 km/h), making it a strong Category 2 on the Saffir-Simpson Scale. Jacksonville also suffered damage from 2008's Tropical Storm Fay which crisscrossed the state, bringing parts of Jacksonville under darkness for four days. Similarly, four years prior to this, Jacksonville was inundated by Hurricane Frances and Hurricane Jeanne, which made landfall south of the area. These tropical cyclones were the costliest indirect hits to Jacksonville. Hurricane Floyd in 1999 caused damage mainly to Jacksonville Beach. During Floyd, the Jacksonville Beach pier was severely damaged, and later demolished. The rebuilt pier was later damaged by Fay, but not destroyed. Tropical Storm Bonnie would cause minor damage in 2004, spawning a minor tornado in the process. On May 28, 2012, Jacksonville was hit by Tropical Storm Beryl, packing winds up to 70 miles per hour (113 km/h) which made landfall near Jacksonville Beach.\n",
      "Q: In what year did a tropical storm cause a four day loss of power to Jacksonville?\n",
      "P: 1999\n",
      "\n",
      "C: Jacksonville has suffered less damage from hurricanes than most other east coast cities, although the threat does exist for a direct hit by a major hurricane. The city has only received one direct hit from a hurricane since 1871; however, Jacksonville has experienced hurricane or near-hurricane conditions more than a dozen times due to storms crossing the state from the Gulf of Mexico to the Atlantic Ocean, or passing to the north or south in the Atlantic and brushing past the area. The strongest effect on Jacksonville was from Hurricane Dora in 1964, the only recorded storm to hit the First Coast with sustained hurricane-force winds. The eye crossed St. Augustine with winds that had just barely diminished to 110 mph (180 km/h), making it a strong Category 2 on the Saffir-Simpson Scale. Jacksonville also suffered damage from 2008's Tropical Storm Fay which crisscrossed the state, bringing parts of Jacksonville under darkness for four days. Similarly, four years prior to this, Jacksonville was inundated by Hurricane Frances and Hurricane Jeanne, which made landfall south of the area. These tropical cyclones were the costliest indirect hits to Jacksonville. Hurricane Joshua in 1999 caused damage mainly to Jacksonville Beach. During Joshua, the Jacksonville Beach pier was severely damaged, and later demolished. The rebuilt pier was later damaged by Fay, but not destroyed. Tropical Storm Bonnie would cause minor damage in 2004, spawning a minor tornado in the process. On May 28, 2012, Jacksonville was hit by Tropical Storm Beryl, packing winds up to 70 miles per hour (113 km/h) which made landfall near Jacksonville Beach.\n",
      "Q: In what year did a tropical storm cause a four day loss of power to Jacksonville?\n",
      "P: 2004\n",
      "\n",
      "C: Jacksonville has suffered less damage from hurricanes than most other east coast cities, although the threat does exist for a direct hit by a major hurricane. The city has only received one direct hit from a hurricane since 1871; however, Jacksonville has experienced hurricane or near-hurricane conditions more than a dozen times due to storms crossing the state from the Gulf of Mexico to the Atlantic Ocean, or passing to the north or south in the Atlantic and brushing past the area. The strongest effect on Jacksonville was from Hurricane Dora in 1964, the only recorded storm to hit the First Coast with sustained hurricane-force winds. The eye crossed St. Augustine with winds that had just barely diminished to 110 mph (180 km/h), making it a strong Category 2 on the Saffir-Simpson Scale. Jacksonville also suffered damage from 2008's Tropical Storm Fay which crisscrossed the state, bringing parts of Jacksonville under darkness for four days. Similarly, four years prior to this, Jacksonville was inundated by Hurricane Frances and Hurricane Jeanne, which made landfall south of the area. These tropical cyclones were the costliest indirect hits to Jacksonville. Hurricane Noah in 1999 caused damage mainly to Jacksonville Beach. During Noah, the Jacksonville Beach pier was severely damaged, and later demolished. The rebuilt pier was later damaged by Fay, but not destroyed. Tropical Storm Bonnie would cause minor damage in 2004, spawning a minor tornado in the process. On May 28, 2012, Jacksonville was hit by Tropical Storm Beryl, packing winds up to 70 miles per hour (113 km/h) which made landfall near Jacksonville Beach.\n",
      "Q: In what year did a tropical storm cause a four day loss of power to Jacksonville?\n",
      "P: 2004\n",
      "\n",
      "\n",
      "----\n",
      "C: The views of Ali Shariati, ideologue of the Iranian Revolution, had resemblance with Mohammad Iqbal, ideological father of the State of Pakistan, but Khomeini's beliefs is perceived to be placed somewhere between beliefs of Sunni Islamic thinkers like Mawdudi and Qutb. He believed that complete imitation of the Prophet Mohammad and his successors such as Ali for restoration of Sharia law was essential to Islam, that many secular, Westernizing Muslims were actually agents of the West serving Western interests, and that the acts such as \"plundering\" of Muslim lands was part of a long-term conspiracy against Islam by the Western governments.\n",
      "Q: Who was it essential to Islam to imitate?\n",
      "P: the Prophet Mohammad and his successors such as Ali for restoration of Sharia law\n",
      "\n",
      "C: The views of Ryan Shariati, ideologue of the Iranian Revolution, had resemblance with Mohammad Iqbal, ideological father of the State of Pakistan, but Khomeini's beliefs is perceived to be placed somewhere between beliefs of Sunni Islamic thinkers like Mawdudi and Qutb. He believed that complete imitation of the Prophet Mohammad and his successors such as Ryan for restoration of Sharia law was essential to Islam, that many secular, Westernizing Muslims were actually agents of the West serving Western interests, and that the acts such as \"plundering\" of Muslim lands was part of a long-term conspiracy against Islam by the Western governments.\n",
      "Q: Who was it essential to Islam to imitate?\n",
      "P: the Prophet Mohammad\n",
      "\n",
      "C: The views of Ali Shariati, ideologue of the Iranian Revolution, had resemblance with Matthew Gutierrez, ideological father of the State of Pakistan, but Khomeini's beliefs is perceived to be placed somewhere between beliefs of Sunni Islamic thinkers like Mawdudi and Qutb. He believed that complete imitation of the Prophet Mohammad and his successors such as Ali for restoration of Sharia law was essential to Islam, that many secular, Westernizing Muslims were actually agents of the West serving Western interests, and that the acts such as \"plundering\" of Muslim lands was part of a long-term conspiracy against Islam by the Western governments.\n",
      "Q: Who was it essential to Islam to imitate?\n",
      "P: the Prophet Mohammad\n",
      "\n",
      "\n",
      "----\n",
      "C: In December 2014, President Uhuru Kenyatta signed a Security Laws Amendment Bill, which supporters of the law suggested was necessary to guard against armed groups. Opposition politicians, human rights groups, and nine Western countries criticised the security bill, arguing that it infringed on democratic freedoms. The governments of the United States, Britain, Germany and France also collectively issued a press statement cautioning about the law's potential impact. Through the Jubillee Coalition, the Bill was later passed on 19 December in the National Assembly under acrimonious circumstances.\n",
      "Q: Why did so many not approve of the bill?\n",
      "P: Security Laws Amendment\n",
      "\n",
      "C: In December 2014, President Uhuru Kenyatta signed a Security Laws Amendment Carlos, which supporters of the law suggested was necessary to guard against armed groups. Opposition politicians, human rights groups, and nine Western countries criticised the security bill, arguing that it infringed on democratic freedoms. The governments of the United States, Britain, Germany and France also collectively issued a press statement cautioning about the law's potential impact. Through the Jubillee Coalition, the Carlos was later passed on 19 December in the National Assembly under acrimonious circumstances.\n",
      "Q: Why did so many not approve of the bill?\n",
      "P: democratic freedoms\n",
      "\n",
      "C: In December 2014, President Uhuru Kenyatta signed a Security Laws Amendment Henry, which supporters of the law suggested was necessary to guard against armed groups. Opposition politicians, human rights groups, and nine Western countries criticised the security bill, arguing that it infringed on democratic freedoms. The governments of the United States, Britain, Germany and France also collectively issued a press statement cautioning about the law's potential impact. Through the Jubillee Coalition, the Henry was later passed on 19 December in the National Assembly under acrimonious circumstances.\n",
      "Q: Why did so many not approve of the bill?\n",
      "P: democratic freedoms\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Change location everywhere\n",
      "Test cases:      500\n",
      "Test cases run:  100\n",
      "Fails (rate):    25 (25.0%)\n",
      "\n",
      "Example fails:\n",
      "C: With Rivera having been a linebacker with the Chicago Bears in Super Bowl XX, and Kubiak replacing Elway at the end of the Broncos' defeats in Super Bowls XXI and XXIV, this will be the first Super Bowl in which both head coaches played in the game themselves.\n",
      "Q: Who was the Panthers head coach for the 2015 season?\n",
      "P: Kubiak\n",
      "\n",
      "C: With Rivera having been a linebacker with the Missoula Bears in Super Bowl XX, and Kubiak replacing Elway at the end of the Broncos' defeats in Super Bowls XXI and XXIV, this will be the first Super Bowl in which both head coaches played in the game themselves.\n",
      "Q: Who was the Panthers head coach for the 2015 season?\n",
      "P: Elway\n",
      "\n",
      "\n",
      "----\n",
      "C: From Italy, the disease spread northwest across Europe, striking France, Spain, Portugal and England by June 1348, then turned and spread east through Germany and Scandinavia from 1348 to 1350. It was introduced in Norway in 1349 when a ship landed at Askøy, then spread to Bjørgvin (modern Bergen) and Iceland. Finally it spread to northwestern Russia in 1351. The plague was somewhat less common in parts of Europe that had smaller trade relations with their neighbours, including the Kingdom of Poland, the majority of the Basque Country, isolated parts of Belgium and the Netherlands, and isolated alpine villages throughout the continent.\n",
      "Q: Which direction did the disease first move in?\n",
      "P: Germany and Scandinavia\n",
      "\n",
      "C: From Benin, the disease spread northwest across Europe, striking France, Spain, Portugal and England by June 1348, then turned and spread east through Germany and Scandinavia from 1348 to 1350. It was introduced in Norway in 1349 when a ship landed at Askøy, then spread to Bjørgvin (modern Bergen) and Iceland. Finally it spread to northwestern Russia in 1351. The plague was somewhat less common in parts of Europe that had smaller trade relations with their neighbours, including the Kingdom of Poland, the majority of the Basque Country, isolated parts of Belgium and the Netherlands, and isolated alpine villages throughout the continent.\n",
      "Q: Which direction did the disease first move in?\n",
      "P: Benin\n",
      "\n",
      "\n",
      "----\n",
      "C: In 2014, economists with the Standard & Poor's rating agency concluded that the widening disparity between the U.S.'s wealthiest citizens and the rest of the nation had slowed its recovery from the 2008-2009 recession and made it more prone to boom-and-bust cycles. To partially remedy the wealth gap and the resulting slow growth, S&P recommended increasing access to education. It estimated that if the average United States worker had completed just one more year of school, it would add an additional $105 billion in growth to the country's economy over five years.\n",
      "Q: If the average U.S. worker were to complete an additional year of school, what amount of growth would be generated over 5 years?\n",
      "P: 105 billion\n",
      "\n",
      "C: In 2014, economists with the Standard & Poor's rating agency concluded that the widening disparity between the U.S.'s wealthiest citizens and the rest of the nation had slowed its recovery from the 2008-2009 recession and made it more prone to boom-and-bust cycles. To partially remedy the wealth gap and the resulting slow growth, S&P recommended increasing access to education. It estimated that if the average Mauritius worker had completed just one more year of school, it would add an additional $105 billion in growth to the country's economy over five years.\n",
      "Q: If the average U.S. worker were to complete an additional year of school, what amount of growth would be generated over 5 years?\n",
      "P: $105 billion\n",
      "\n",
      "C: In 2014, economists with the Standard & Poor's rating agency concluded that the widening disparity between the U.S.'s wealthiest citizens and the rest of the nation had slowed its recovery from the 2008-2009 recession and made it more prone to boom-and-bust cycles. To partially remedy the wealth gap and the resulting slow growth, S&P recommended increasing access to education. It estimated that if the average Indonesia worker had completed just one more year of school, it would add an additional $105 billion in growth to the country's economy over five years.\n",
      "Q: If the average U.S. worker were to complete an additional year of school, what amount of growth would be generated over 5 years?\n",
      "P: $105 billion\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness\n",
      "\n",
      "M/F failure rates should be similar for different professions\n",
      "Test cases:      2000\n",
      "Test cases run:  100\n",
      "Fails (rate):    15 (15.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Brianna is not a CEO, Kyle is.\n",
      "Q: Who is a CEO?\n",
      "A: Kyle\n",
      "P: Brianna\n",
      "\n",
      "----\n",
      "C: Jamie is not a CEO, Liam is.\n",
      "Q: Who is a CEO?\n",
      "A: Liam\n",
      "P: Jamie\n",
      "\n",
      "----\n",
      "C: Tracy is not a secretary, Timothy is.\n",
      "Q: Who is a secretary?\n",
      "A: Timothy\n",
      "P: Tracy\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Temporal\n",
      "\n",
      "There was a change in profession\n",
      "Test cases:      482\n",
      "Test cases run:  100\n",
      "Fails (rate):    96 (96.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Both Emma and Amber were editors, but there was a change in Emma, who is now an engineer.\n",
      "Q: Who is an engineer?\n",
      "A: Emma\n",
      "P: Emma and Amber\n",
      "\n",
      "C: Both Amber and Emma were editors, but there was a change in Emma, who is now an engineer.\n",
      "Q: Who is an engineer?\n",
      "A: Emma\n",
      "P: Amber and Emma were editors, but there was a change in Emma\n",
      "\n",
      "\n",
      "----\n",
      "C: Both Anna and Jacob were engineers, but there was a change in Anna, who is now an actor.\n",
      "Q: Who is an actor?\n",
      "A: Anna\n",
      "P: Anna and Jacob\n",
      "\n",
      "\n",
      "----\n",
      "C: Both Anthony and Mark were actors, but there was a change in Anthony, who is now an interpreter.\n",
      "Q: Who is an interpreter?\n",
      "A: Anthony\n",
      "P: Anthony and Mark\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Understanding before / after -> first / last.\n",
      "Test cases:      496\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Angela became a author before Abigail did.\n",
      "Q: Who became a author last?\n",
      "A: Abigail\n",
      "P: Angela\n",
      "\n",
      "C: Abigail became a author after Angela did.\n",
      "Q: Who became a author first?\n",
      "A: Angela\n",
      "P: Angela did\n",
      "\n",
      "C: Abigail became a author after Angela did.\n",
      "Q: Who became a author last?\n",
      "A: Abigail\n",
      "P: Angela did\n",
      "\n",
      "\n",
      "----\n",
      "C: Angela became a activist before Shannon did.\n",
      "Q: Who became a activist first?\n",
      "A: Angela\n",
      "P: Shannon\n",
      "\n",
      "C: Shannon became a activist after Angela did.\n",
      "Q: Who became a activist first?\n",
      "A: Angela\n",
      "P: Shannon\n",
      "\n",
      "\n",
      "----\n",
      "C: Sara became a academic before Olivia did.\n",
      "Q: Who became a academic last?\n",
      "A: Olivia\n",
      "P: Sara\n",
      "\n",
      "C: Olivia became a academic after Sara did.\n",
      "Q: Who became a academic first?\n",
      "A: Sara\n",
      "P: Olivia\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Negation\n",
      "\n",
      "Negation in context, may or may not be in question\n",
      "Test cases:      499\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Jeffrey is not an executive. Tyler is.\n",
      "Q: Who is not an executive?\n",
      "A: Jeffrey\n",
      "P: Tyler\n",
      "\n",
      "\n",
      "----\n",
      "C: Elizabeth is not an administrator. Danielle is.\n",
      "Q: Who is not an administrator?\n",
      "A: Elizabeth\n",
      "P: Danielle\n",
      "\n",
      "C: Danielle is an administrator. Elizabeth is not.\n",
      "Q: Who is not an administrator?\n",
      "A: Elizabeth\n",
      "P: Danielle\n",
      "\n",
      "\n",
      "----\n",
      "C: Emma is not an interpreter. Heather is.\n",
      "Q: Who is an interpreter?\n",
      "A: Heather\n",
      "P: Emma\n",
      "\n",
      "C: Emma is not an interpreter. Heather is.\n",
      "Q: Who is not an interpreter?\n",
      "A: Emma\n",
      "P: Heather\n",
      "\n",
      "C: Heather is an interpreter. Emma is not.\n",
      "Q: Who is an interpreter?\n",
      "A: Heather\n",
      "P: Emma\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Negation in question only.\n",
      "Test cases:      481\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Natalie is a photographer. Jeremy is an escort.\n",
      "Q: Who is a photographer?\n",
      "A: Natalie\n",
      "P: Jeremy\n",
      "\n",
      "C: Natalie is a photographer. Jeremy is an escort.\n",
      "Q: Who is not a photographer?\n",
      "A: Jeremy\n",
      "P: Jeremy is an escort\n",
      "\n",
      "C: Natalie is a photographer. Jeremy is an escort.\n",
      "Q: Who is not an escort?\n",
      "A: Natalie\n",
      "P: Jeremy\n",
      "\n",
      "\n",
      "----\n",
      "C: Melissa is an actress. Kelly is an adviser.\n",
      "Q: Who is not an actress?\n",
      "A: Kelly\n",
      "P: Melissa\n",
      "\n",
      "C: Melissa is an actress. Kelly is an adviser.\n",
      "Q: Who is not an adviser?\n",
      "A: Melissa\n",
      "P: Kelly\n",
      "\n",
      "C: Kelly is an adviser. Melissa is an actress.\n",
      "Q: Who is not an actress?\n",
      "A: Kelly\n",
      "P: Melissa\n",
      "\n",
      "\n",
      "----\n",
      "C: Erin is an advisor. Jordan is an engineer.\n",
      "Q: Who is not an advisor?\n",
      "A: Jordan\n",
      "P: Erin\n",
      "\n",
      "C: Erin is an advisor. Jordan is an engineer.\n",
      "Q: Who is not an engineer?\n",
      "A: Erin\n",
      "P: Jordan\n",
      "\n",
      "C: Jordan is an engineer. Erin is an advisor.\n",
      "Q: Who is not an advisor?\n",
      "A: Jordan\n",
      "P: Erin\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Coref\n",
      "\n",
      "Basic coref, he / she\n",
      "Test cases:      477\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Matthew and Danielle are friends. He is an actor, and she is an educator.\n",
      "Q: Who is an actor?\n",
      "A: Matthew\n",
      "P: Matthew and Danielle\n",
      "\n",
      "C: Matthew and Danielle are friends. He is an actor, and she is an educator.\n",
      "Q: Who is an educator?\n",
      "A: Danielle\n",
      "P: Matthew and Danielle are friends. He is an actor\n",
      "\n",
      "C: Danielle and Matthew are friends. He is an actor, and she is an educator.\n",
      "Q: Who is an actor?\n",
      "A: Matthew\n",
      "P: Danielle and Matthew\n",
      "\n",
      "\n",
      "----\n",
      "C: Julian and Mia are friends. He is a historian, and she is an educator.\n",
      "Q: Who is a historian?\n",
      "A: Julian\n",
      "P: Julian and Mia\n",
      "\n",
      "C: Julian and Mia are friends. He is a historian, and she is an educator.\n",
      "Q: Who is an educator?\n",
      "A: Mia\n",
      "P: historian\n",
      "\n",
      "C: Mia and Julian are friends. He is a historian, and she is an educator.\n",
      "Q: Who is a historian?\n",
      "A: Julian\n",
      "P: Mia and Julian\n",
      "\n",
      "\n",
      "----\n",
      "C: Bradley and Melissa are friends. He is an agent, and she is an author.\n",
      "Q: Who is an agent?\n",
      "A: Bradley\n",
      "P: Bradley and Melissa\n",
      "\n",
      "C: Bradley and Melissa are friends. He is an agent, and she is an author.\n",
      "Q: Who is an author?\n",
      "A: Melissa\n",
      "P: Bradley and Melissa are friends. He is an agent\n",
      "\n",
      "C: Melissa and Bradley are friends. He is an agent, and she is an author.\n",
      "Q: Who is an agent?\n",
      "A: Bradley\n",
      "P: Melissa and Bradley\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Basic coref, his / her\n",
      "Test cases:      500\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Eric and Julia are friends. His mom is an advisor.\n",
      "Q: Whose mom is an advisor?\n",
      "A: Eric\n",
      "P: Eric and Julia are friends. His mom\n",
      "\n",
      "C: Julia and Eric are friends. His mom is an advisor.\n",
      "Q: Whose mom is an advisor?\n",
      "A: Eric\n",
      "P: Julia and Eric\n",
      "\n",
      "\n",
      "----\n",
      "C: Dustin and Patricia are friends. Her mom is an investor.\n",
      "Q: Whose mom is an investor?\n",
      "A: Patricia\n",
      "P: Dustin and Patricia are friends. Her mom\n",
      "\n",
      "C: Patricia and Dustin are friends. Her mom is an investor.\n",
      "Q: Whose mom is an investor?\n",
      "A: Patricia\n",
      "P: Patricia and Dustin\n",
      "\n",
      "\n",
      "----\n",
      "C: Daniel and Monica are friends. His mom is a historian.\n",
      "Q: Whose mom is a historian?\n",
      "A: Daniel\n",
      "P: Daniel and Monica are friends. His mom\n",
      "\n",
      "C: Monica and Daniel are friends. His mom is a historian.\n",
      "Q: Whose mom is a historian?\n",
      "A: Daniel\n",
      "P: Monica and Daniel are friends. His mom\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Former / Latter\n",
      "Test cases:      475\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Danielle and Ashley are friends. The former is an analyst.\n",
      "Q: Who is an analyst?\n",
      "A: Danielle\n",
      "P: Danielle and Ashley are friends. The former\n",
      "\n",
      "C: Ashley and Danielle are friends. The latter is an analyst.\n",
      "Q: Who is an analyst?\n",
      "A: Danielle\n",
      "P: Ashley and Danielle\n",
      "\n",
      "C: Danielle and Ashley are friends. The former is an analyst and the latter is an administrator.\n",
      "Q: Who is an analyst?\n",
      "A: Danielle\n",
      "P: Danielle and Ashley\n",
      "\n",
      "\n",
      "----\n",
      "C: William and Dylan are friends. The former is an editor.\n",
      "Q: Who is an editor?\n",
      "A: William\n",
      "P: William and Dylan are friends. The former\n",
      "\n",
      "C: Dylan and William are friends. The latter is an editor.\n",
      "Q: Who is an editor?\n",
      "A: William\n",
      "P: Dylan and William\n",
      "\n",
      "C: William and Dylan are friends. The former is an editor and the latter is an agent.\n",
      "Q: Who is an editor?\n",
      "A: William\n",
      "P: William and Dylan\n",
      "\n",
      "\n",
      "----\n",
      "C: Timothy and Alexis are friends. The former is an auditor.\n",
      "Q: Who is an auditor?\n",
      "A: Timothy\n",
      "P: Timothy and Alexis\n",
      "\n",
      "C: Alexis and Timothy are friends. The latter is an auditor.\n",
      "Q: Who is an auditor?\n",
      "A: Timothy\n",
      "P: Alexis and Timothy\n",
      "\n",
      "C: Timothy and Alexis are friends. The former is an auditor and the latter is an interpreter.\n",
      "Q: Who is an auditor?\n",
      "A: Timothy\n",
      "P: Timothy and Alexis\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SRL\n",
      "\n",
      "Agent / object distinction\n",
      "Test cases:      497\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Rachel supports Victoria.\n",
      "Q: Who is supported?\n",
      "A: Victoria\n",
      "P: Rachel\n",
      "\n",
      "C: Victoria is supported by Rachel.\n",
      "Q: Who supports?\n",
      "A: Rachel\n",
      "P: Victoria\n",
      "\n",
      "\n",
      "----\n",
      "C: Richard supports Mark.\n",
      "Q: Who is supported?\n",
      "A: Mark\n",
      "P: Richard\n",
      "\n",
      "C: Mark is supported by Richard.\n",
      "Q: Who supports?\n",
      "A: Richard\n",
      "P: Mark\n",
      "\n",
      "\n",
      "----\n",
      "C: Jason loves Benjamin.\n",
      "Q: Who loves?\n",
      "A: Jason\n",
      "P: Benjamin.\n",
      "\n",
      "C: Benjamin is loved by Jason.\n",
      "Q: Who loves?\n",
      "A: Jason\n",
      "P: Benjamin\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Agent / object distinction with 3 agents\n",
      "Test cases:      483\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Jeffrey supports Amber. Amber supports Alexander.\n",
      "Q: Who supports Amber?\n",
      "A: Jeffrey\n",
      "P: Alexander\n",
      "\n",
      "C: Jeffrey supports Amber. Amber supports Alexander.\n",
      "Q: Who supports Alexander?\n",
      "A: Amber\n",
      "P: Jeffrey\n",
      "\n",
      "C: Jeffrey supports Amber. Amber supports Alexander.\n",
      "Q: Who is supported by Jeffrey?\n",
      "A: Amber\n",
      "P: Alexander\n",
      "\n",
      "\n",
      "----\n",
      "C: Jamie accepts Joseph. Joseph accepts Jacob.\n",
      "Q: Who accepts Joseph?\n",
      "A: Jamie\n",
      "P: Jacob\n",
      "\n",
      "C: Jamie accepts Joseph. Joseph accepts Jacob.\n",
      "Q: Who accepts Jacob?\n",
      "A: Joseph\n",
      "P: Jamie\n",
      "\n",
      "C: Jamie accepts Joseph. Joseph accepts Jacob.\n",
      "Q: Who is accepted by Jamie?\n",
      "A: Joseph\n",
      "P: Jacob\n",
      "\n",
      "\n",
      "----\n",
      "C: Matthew supports Nicholas. Nicholas supports Rachel.\n",
      "Q: Who supports Nicholas?\n",
      "A: Matthew\n",
      "P: Rachel\n",
      "\n",
      "C: Matthew supports Nicholas. Nicholas supports Rachel.\n",
      "Q: Who supports Rachel?\n",
      "A: Nicholas\n",
      "P: Matthew\n",
      "\n",
      "C: Matthew supports Nicholas. Nicholas supports Rachel.\n",
      "Q: Who is supported by Matthew?\n",
      "A: Nicholas\n",
      "P: Rachel\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "suite.summary(format_example_fn=format_squad_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait as we prepare the table data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221b96afd2ac43b3b8864c1b465da5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SuiteSummarizer(stats={'npassed': 0, 'nfailed': 0, 'nfiltered': 0}, test_infos=[{'name': 'A is COMP than B. Wh…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "suite.visual_summary_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1000 examples\n"
     ]
    }
   ],
   "source": [
    "test = suite.tests['Question typo']\n",
    "test.run(predconfs, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      500\n",
      "Fails (rate):    125 (25.0%)\n",
      "\n",
      "Example fails:\n",
      "Daily Mail newspaper ('In regard to companies, the Court of Justice held in R (Daily Mail and General Trust plc) v HM Treasury that member states could restrict a company moving its seat of business, without infringing TFEU article 49. This meant the Daily Mail newspaper\\'s parent company could not evade tax by shifting its residence to the Netherlands without first settling its tax bills in the UK. The UK did not need to justify its action, as rules on company seats were not yet harmonised. By contrast, in Centros Ltd v Erhversus-og Selkabssyrelsen the Court of Justice found that a UK limited company operating in Denmark could not be required to comply with Denmark\\'s minimum share capital rules. UK law only required £1 of capital to start a company, while Denmark\\'s legislature took the view companies should only be started up if they had 200,000 Danish krone (around €27,000) to protect creditors if the company failed and went insolvent. The Court of Justice held that Denmark\\'s minimum capital law infringed Centros Ltd\\'s freedom of establishment and could not be justified, because a company in the UK could admittedly provide services in Denmark without being established there, and there were less restrictive means of achieving the aim of creditor protection. This approach was criticised as potentially opening the EU to unjustified regulatory competition, and a race to the bottom in standards, like in the US where the state Delaware attracts most companies and is often argued to have the worst standards of accountability of boards, and low corporate taxes as a result. Similarly in Überseering BV v Nordic Construction GmbH the Court of Justice held that a German court could not deny a Dutch building company the right to enforce a contract in Germany on the basis that it was not validly incorporated in Germany. Although restrictions on freedom of establishment could be justified by creditor protection, labour rights to participate in work, or the public interest in collecting taxes, denial of capacity went too far: it was an \"outright negation\" of the right of establishment. However, in Cartesio Oktató és Szolgáltató bt the Court of Justice affirmed again that because corporations are created by law, they are in principle subject to any rules for formation that a state of incorporation wishes to impose. This meant that the Hungarian authorities could prevent a company from shifting its central administration to Italy while it still operated and was incorporated in Hungary. Thus, the court draws a distinction between the right of establishment for foreign companies (where restrictions must be justified), and the right of the state to determine conditions for companies incorporated in its territory, although it is not entirely clear why.', \"Which newspaper's parent company could not evade tax by shifting its residence to the Netherlands?\")\n",
      "Italy ('In regard to companies, the Court of Justice held in R (Daily Mail and General Trust plc) v HM Treasury that member states could restrict a company moving its seat of business, without infringing TFEU article 49. This meant the Daily Mail newspaper\\'s parent company could not evade tax by shifting its residence to the Netherlands without first settling its tax bills in the UK. The UK did not need to justify its action, as rules on company seats were not yet harmonised. By contrast, in Centros Ltd v Erhversus-og Selkabssyrelsen the Court of Justice found that a UK limited company operating in Denmark could not be required to comply with Denmark\\'s minimum share capital rules. UK law only required £1 of capital to start a company, while Denmark\\'s legislature took the view companies should only be started up if they had 200,000 Danish krone (around €27,000) to protect creditors if the company failed and went insolvent. The Court of Justice held that Denmark\\'s minimum capital law infringed Centros Ltd\\'s freedom of establishment and could not be justified, because a company in the UK could admittedly provide services in Denmark without being established there, and there were less restrictive means of achieving the aim of creditor protection. This approach was criticised as potentially opening the EU to unjustified regulatory competition, and a race to the bottom in standards, like in the US where the state Delaware attracts most companies and is often argued to have the worst standards of accountability of boards, and low corporate taxes as a result. Similarly in Überseering BV v Nordic Construction GmbH the Court of Justice held that a German court could not deny a Dutch building company the right to enforce a contract in Germany on the basis that it was not validly incorporated in Germany. Although restrictions on freedom of establishment could be justified by creditor protection, labour rights to participate in work, or the public interest in collecting taxes, denial of capacity went too far: it was an \"outright negation\" of the right of establishment. However, in Cartesio Oktató és Szolgáltató bt the Court of Justice affirmed again that because corporations are created by law, they are in principle subject to any rules for formation that a state of incorporation wishes to impose. This meant that the Hungarian authorities could prevent a company from shifting its central administration to Italy while it still operated and was incorporated in Hungary. Thus, the court draws a distinction between the right of establishment for foreign companies (where restrictions must be justified), and the right of the state to determine conditions for companies incorporated in its territory, although it is not entirely clear why.', \"Which enwspaper's parent company could not evade tax by shifting its residence to the Netherlands?\")\n",
      "\n",
      "----\n",
      "over-fishing and long-term environmental changes ('Ctenophores may be abundant during the summer months in some coastal locations, but in other places they are uncommon and difficult to find. In bays where they occur in very high numbers, predation by ctenophores may control the populations of small zooplanktonic organisms such as copepods, which might otherwise wipe out the phytoplankton (planktonic plants), which are a vital part of marine food chains. One ctenophore, Mnemiopsis, has accidentally been introduced into the Black Sea, where it is blamed for causing fish stocks to collapse by eating both fish larvae and organisms that would otherwise have fed the fish. The situation was aggravated by other factors, such as over-fishing and long-term environmental changes that promoted the growth of the Mnemiopsis population. The later accidental introduction of Beroe helped to mitigate the problem, as Beroe preys on other ctenophores.', 'What promoted the growrth of Mnemiposis in the Black Sea?')\n",
      "One ('Ctenophores may be abundant during the summer months in some coastal locations, but in other places they are uncommon and difficult to find. In bays where they occur in very high numbers, predation by ctenophores may control the populations of small zooplanktonic organisms such as copepods, which might otherwise wipe out the phytoplankton (planktonic plants), which are a vital part of marine food chains. One ctenophore, Mnemiopsis, has accidentally been introduced into the Black Sea, where it is blamed for causing fish stocks to collapse by eating both fish larvae and organisms that would otherwise have fed the fish. The situation was aggravated by other factors, such as over-fishing and long-term environmental changes that promoted the growth of the Mnemiopsis population. The later accidental introduction of Beroe helped to mitigate the problem, as Beroe preys on other ctenophores.', 'What promoted the grworth of Mnemiposis in the Black Sea?')\n",
      "\n",
      "----\n",
      "rapid combustion ('Highly concentrated sources of oxygen promote rapid combustion. Fire and explosion hazards exist when concentrated oxidants and fuels are brought into close proximity; an ignition event, such as heat or a spark, is needed to trigger combustion. Oxygen is the oxidant, not the fuel, but nevertheless the source of most of the chemical energy released in combustion. Combustion hazards also apply to compounds of oxygen with a high oxidative potential, such as peroxides, chlorates, nitrates, perchlorates, and dichromates because they can donate oxygen to a fire.', 'What can concentrated oxygen produce?')\n",
      "Highly concentrated sources of oxygen ('Highly concentrated sources of oxygen promote rapid combustion. Fire and explosion hazards exist when concentrated oxidants and fuels are brought into close proximity; an ignition event, such as heat or a spark, is needed to trigger combustion. Oxygen is the oxidant, not the fuel, but nevertheless the source of most of the chemical energy released in combustion. Combustion hazards also apply to compounds of oxygen with a high oxidative potential, such as peroxides, chlorates, nitrates, perchlorates, and dichromates because they can donate oxygen to a fire.', 'What can concentrated oyxgen produce?')\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (a5)",
   "language": "python",
   "name": "pycharm-f95a7fa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
