{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e833e60d",
   "metadata": {},
   "source": [
    "# MLM training on our generated data\n",
    "\n",
    "adapted from\n",
    "https://huggingface.co/transformers/v2.5.1/examples.html#language-model-training\n",
    "and\n",
    "https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d45a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForWholeWordMask,\n",
    "    DataCollatorWithPadding,\n",
    "    HfArgumentParser,\n",
    "    LineByLineTextDataset,\n",
    "    LineByLineWithRefDataset,\n",
    "    PreTrainedTokenizer,\n",
    "    TextDataset,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    pipeline\n",
    ")\n",
    "from transformers.trainer_utils import is_main_process\n",
    "import datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "563351c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE = './hfcache'\n",
    "MODEL_NAME = 'google/electra-small-generator'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb1ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(MODEL_NAME, cache_dir=CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "589347d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99efb367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 128, padding_idx=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    from_tf=False,\n",
    "    config=config,\n",
    "    cache_dir=CACHE,\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc3453c",
   "metadata": {},
   "source": [
    "# build new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f0ff969",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./contrast_dataset/data.csv', index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83d90159",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = datasets.Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "394ab44e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['x', 'y', '__index_level_0__'],\n",
       "    num_rows: 112436\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "443ce64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I thought Kevin tends to be excessively [MASK], but  he was very old.',\n",
       " 'young')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['x'][0], d['y'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df227b8d",
   "metadata": {},
   "source": [
    "### A. tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d83d432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"x\"], max_length=512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "347ac619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function_y(examples):\n",
    "    return tokenizer(examples[\"y\"], max_length=1, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee39154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = d.map(tokenize_function, batched=True, num_proc=4, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec61ad2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5553b46e59b64209bf4d5c92aa5ccd58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = d.map(tokenize_function_y, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dfa1659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# something about arrow format makes it insanely slow on multiple reads\n",
    "ys = list(y['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "067da8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = [y[1] for y in ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76a6f6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2402,\n",
       " 3893,\n",
       " 9191,\n",
       " 2658,\n",
       " 5236,\n",
       " 6638,\n",
       " 3563,\n",
       " 3532,\n",
       " 2844,\n",
       " 3161,\n",
       " 4603,\n",
       " 15716,\n",
       " 7790,\n",
       " 5305,\n",
       " 3161,\n",
       " 2204,\n",
       " 6450,\n",
       " 2397,\n",
       " 6450,\n",
       " 2179,\n",
       " 9841,\n",
       " 21877,\n",
       " 17772,\n",
       " 19194,\n",
       " 13593,\n",
       " 2980,\n",
       " 5776,\n",
       " 3491,\n",
       " 8841,\n",
       " 2986,\n",
       " 8796,\n",
       " 2047,\n",
       " 6530,\n",
       " 14480,\n",
       " 3893,\n",
       " 2047,\n",
       " 5379,\n",
       " 4251,\n",
       " 2658,\n",
       " 4621,\n",
       " 4064,\n",
       " 22692,\n",
       " 5665,\n",
       " 4206,\n",
       " 15716,\n",
       " 3407,\n",
       " 9191,\n",
       " 2304,\n",
       " 2485,\n",
       " 7790,\n",
       " 4064,\n",
       " 9200,\n",
       " 5305,\n",
       " 6047,\n",
       " 3893,\n",
       " 7501,\n",
       " 3819,\n",
       " 2214,\n",
       " 17145,\n",
       " 3532,\n",
       " 4206,\n",
       " 2092,\n",
       " 4138,\n",
       " 2214,\n",
       " 4550,\n",
       " 8796,\n",
       " 7098,\n",
       " 2235,\n",
       " 2986,\n",
       " 2797,\n",
       " 11004,\n",
       " 12511,\n",
       " 7501,\n",
       " 7568,\n",
       " 2066,\n",
       " 7823,\n",
       " 2402,\n",
       " 3161,\n",
       " 2986,\n",
       " 4603,\n",
       " 3532,\n",
       " 5305,\n",
       " 4121,\n",
       " 7591,\n",
       " 2047,\n",
       " 5379,\n",
       " 5220,\n",
       " 12958,\n",
       " 6179,\n",
       " 3809,\n",
       " 3733,\n",
       " 6555,\n",
       " 3733,\n",
       " 4408,\n",
       " 7501,\n",
       " 7823,\n",
       " 4997,\n",
       " 3893,\n",
       " 5236,\n",
       " 21931,\n",
       " 2502,\n",
       " 12511,\n",
       " 2047,\n",
       " 4452,\n",
       " 3161,\n",
       " 12958,\n",
       " 7568,\n",
       " 5341,\n",
       " 4326,\n",
       " 5236,\n",
       " 2917,\n",
       " 3308,\n",
       " 2317,\n",
       " 6638,\n",
       " 3835,\n",
       " 2092,\n",
       " 2986,\n",
       " 12511,\n",
       " 14768,\n",
       " 6555,\n",
       " 21877,\n",
       " 6047,\n",
       " 6530,\n",
       " 11004,\n",
       " 14480,\n",
       " 5024,\n",
       " 2759,\n",
       " 5665,\n",
       " 5665,\n",
       " 2304,\n",
       " 3147,\n",
       " 3697,\n",
       " 4064,\n",
       " 3491,\n",
       " 3893,\n",
       " 10036,\n",
       " 4242,\n",
       " 4795,\n",
       " 2214,\n",
       " 6387,\n",
       " 26103,\n",
       " 26103,\n",
       " 17380,\n",
       " 4251,\n",
       " 10539,\n",
       " 22692,\n",
       " 3697,\n",
       " 12958,\n",
       " 22692,\n",
       " 7968,\n",
       " 17772,\n",
       " 4603,\n",
       " 2402,\n",
       " 4010,\n",
       " 14768,\n",
       " 3733,\n",
       " 22692,\n",
       " 6530,\n",
       " 2980,\n",
       " 13135,\n",
       " 2919,\n",
       " 5236,\n",
       " 2485,\n",
       " 2919,\n",
       " 26103,\n",
       " 5341,\n",
       " 6638,\n",
       " 5710,\n",
       " 3161,\n",
       " 2919,\n",
       " 21931,\n",
       " 17380,\n",
       " 2235,\n",
       " 7965,\n",
       " 5776,\n",
       " 3733,\n",
       " 3161,\n",
       " 21931,\n",
       " 5220,\n",
       " 3647,\n",
       " 5710,\n",
       " 3819,\n",
       " 17380,\n",
       " 12511,\n",
       " 4658,\n",
       " 4242,\n",
       " 3563,\n",
       " 3375,\n",
       " 12511,\n",
       " 26103,\n",
       " 6638,\n",
       " 5776,\n",
       " 21931,\n",
       " 5410,\n",
       " 7098,\n",
       " 13593,\n",
       " 12511,\n",
       " 2485,\n",
       " 2797,\n",
       " 4795,\n",
       " 5305,\n",
       " 4030,\n",
       " 12726,\n",
       " 9191,\n",
       " 5341,\n",
       " 6179,\n",
       " 26438,\n",
       " 9200,\n",
       " 26103,\n",
       " 3308,\n",
       " 2182,\n",
       " 14480,\n",
       " 3375,\n",
       " 4010,\n",
       " 4603,\n",
       " 3491,\n",
       " 5236,\n",
       " 2179,\n",
       " 25192,\n",
       " 6387,\n",
       " 3893,\n",
       " 9191,\n",
       " 3835,\n",
       " 2919,\n",
       " 5600,\n",
       " 2658,\n",
       " 2844,\n",
       " 2397,\n",
       " 6387,\n",
       " 17380,\n",
       " 17145,\n",
       " 26103,\n",
       " 3147,\n",
       " 3143,\n",
       " 4621,\n",
       " 2759,\n",
       " 11809,\n",
       " 2986,\n",
       " 3563,\n",
       " 2980,\n",
       " 4030,\n",
       " 11004,\n",
       " 2179,\n",
       " 4138,\n",
       " 3722,\n",
       " 3308,\n",
       " 2204,\n",
       " 7501,\n",
       " 3532,\n",
       " 9191,\n",
       " 16021,\n",
       " 3532,\n",
       " 8841,\n",
       " 5710,\n",
       " 7965,\n",
       " 6047,\n",
       " 2917,\n",
       " 5220,\n",
       " 7790,\n",
       " 17772,\n",
       " 2714,\n",
       " 3375,\n",
       " 11004,\n",
       " 2402,\n",
       " 3819,\n",
       " 3697,\n",
       " 7591,\n",
       " 7965,\n",
       " 14480,\n",
       " 13205,\n",
       " 9414,\n",
       " 25192,\n",
       " 2919,\n",
       " 16021,\n",
       " 5600,\n",
       " 6832,\n",
       " 14768,\n",
       " 3563,\n",
       " 13205,\n",
       " 16021,\n",
       " 6530,\n",
       " 7501,\n",
       " 15716,\n",
       " 2759,\n",
       " 5236,\n",
       " 15716,\n",
       " 5410,\n",
       " 16021,\n",
       " 12726,\n",
       " 6387,\n",
       " 2317,\n",
       " 12726,\n",
       " 2524,\n",
       " 7098,\n",
       " 2235,\n",
       " 8841,\n",
       " 26103,\n",
       " 13593,\n",
       " 21877,\n",
       " 3893,\n",
       " 2179,\n",
       " 5600,\n",
       " 17380,\n",
       " 25192,\n",
       " 3563,\n",
       " 6832,\n",
       " 5305,\n",
       " 3722,\n",
       " 2092,\n",
       " 3722,\n",
       " 4452,\n",
       " 2502,\n",
       " 11004,\n",
       " 4010,\n",
       " 4603,\n",
       " 2485,\n",
       " 3375,\n",
       " 6530,\n",
       " 4138,\n",
       " 4064,\n",
       " 4030,\n",
       " 3809,\n",
       " 3563,\n",
       " 21446,\n",
       " 2397,\n",
       " 2179,\n",
       " 8841,\n",
       " 21446,\n",
       " 7823,\n",
       " 5236,\n",
       " 5220,\n",
       " 26103,\n",
       " 3161,\n",
       " 5305,\n",
       " 3809,\n",
       " 3407,\n",
       " 14768,\n",
       " 6387,\n",
       " 2919,\n",
       " 2658,\n",
       " 4251,\n",
       " 4550,\n",
       " 11004,\n",
       " 22692,\n",
       " 3733,\n",
       " 6450,\n",
       " 2204,\n",
       " 26103,\n",
       " 7501,\n",
       " 3697,\n",
       " 3161,\n",
       " 2397,\n",
       " 4408,\n",
       " 4138,\n",
       " 4997,\n",
       " 2980,\n",
       " 2235,\n",
       " 6832,\n",
       " 7965,\n",
       " 2397,\n",
       " 14768,\n",
       " 16021,\n",
       " 4251,\n",
       " 3563,\n",
       " 9841,\n",
       " 7965,\n",
       " 4603,\n",
       " 6047,\n",
       " 2524,\n",
       " 5776,\n",
       " 3697,\n",
       " 3819,\n",
       " 6047,\n",
       " 7965,\n",
       " 3697,\n",
       " 3308,\n",
       " 3491,\n",
       " 3647,\n",
       " 4010,\n",
       " 9200,\n",
       " 6638,\n",
       " 9841,\n",
       " 8841,\n",
       " 2919,\n",
       " 2797,\n",
       " 8841,\n",
       " 20868,\n",
       " 5236,\n",
       " 5220,\n",
       " 13593,\n",
       " 3143,\n",
       " 5776,\n",
       " 4010,\n",
       " 19194,\n",
       " 15716,\n",
       " 3147,\n",
       " 15716,\n",
       " 3161,\n",
       " 12726,\n",
       " 3375,\n",
       " 2066,\n",
       " 12958,\n",
       " 3143,\n",
       " 2047,\n",
       " 4138,\n",
       " 12726,\n",
       " 3491,\n",
       " 3532,\n",
       " 3407,\n",
       " 12958,\n",
       " 2524,\n",
       " 9841,\n",
       " 17772,\n",
       " 25192,\n",
       " 3161,\n",
       " 7790,\n",
       " 11004,\n",
       " 4997,\n",
       " 6047,\n",
       " 9200,\n",
       " 2304,\n",
       " 3819,\n",
       " 17145,\n",
       " 3376,\n",
       " 4763,\n",
       " 3143,\n",
       " 4658,\n",
       " 3407,\n",
       " 3265,\n",
       " 14768,\n",
       " 21931,\n",
       " 3893,\n",
       " 7568,\n",
       " 3143,\n",
       " 21877,\n",
       " 8796,\n",
       " 6450,\n",
       " 5600,\n",
       " 9191,\n",
       " 3722,\n",
       " 5236,\n",
       " 3375,\n",
       " 6555,\n",
       " 4795,\n",
       " 3819,\n",
       " 9200,\n",
       " 4452,\n",
       " 3375,\n",
       " 6555,\n",
       " 6387,\n",
       " 3161,\n",
       " 2524,\n",
       " 5236,\n",
       " 6555,\n",
       " 3532,\n",
       " 7790,\n",
       " 2402,\n",
       " 5710,\n",
       " 3161,\n",
       " 14768,\n",
       " 4326,\n",
       " 7098,\n",
       " 16021,\n",
       " 21877,\n",
       " 3893,\n",
       " 5379,\n",
       " 7568,\n",
       " 11434,\n",
       " 6450,\n",
       " 13593,\n",
       " 13135,\n",
       " 4795,\n",
       " 3375,\n",
       " 3722,\n",
       " 2066,\n",
       " 2714,\n",
       " 5710,\n",
       " 12726,\n",
       " 2797,\n",
       " 6555,\n",
       " 2402,\n",
       " 15716,\n",
       " 3563,\n",
       " 4242,\n",
       " 26103,\n",
       " 3697,\n",
       " 2179,\n",
       " 4030,\n",
       " 3375,\n",
       " 26103,\n",
       " 13135,\n",
       " 12511,\n",
       " 2658,\n",
       " 6387,\n",
       " 2800,\n",
       " 2204,\n",
       " 5600,\n",
       " 4550,\n",
       " 13135,\n",
       " 2066,\n",
       " 2047,\n",
       " 2092,\n",
       " 15716,\n",
       " 7823,\n",
       " 4251,\n",
       " 3491,\n",
       " 2919,\n",
       " 4251,\n",
       " 21446,\n",
       " 17380,\n",
       " 6638,\n",
       " 3375,\n",
       " 11434,\n",
       " 3893,\n",
       " 3697,\n",
       " 3893,\n",
       " 2047,\n",
       " 9200,\n",
       " 16021,\n",
       " 3697,\n",
       " 12726,\n",
       " 12958,\n",
       " 5220,\n",
       " 9841,\n",
       " 3161,\n",
       " 17380,\n",
       " 15716,\n",
       " 13205,\n",
       " 4121,\n",
       " 6179,\n",
       " 3491,\n",
       " 6638,\n",
       " 14480,\n",
       " 4138,\n",
       " 15716,\n",
       " 2986,\n",
       " 4550,\n",
       " 3143,\n",
       " 2919,\n",
       " 17772,\n",
       " 13593,\n",
       " 3532,\n",
       " 11434,\n",
       " 13593,\n",
       " 10539,\n",
       " 4658,\n",
       " 9414,\n",
       " 2179,\n",
       " 2844,\n",
       " 2919,\n",
       " 3647,\n",
       " 5236,\n",
       " 7568,\n",
       " 3265,\n",
       " 26438,\n",
       " 7965,\n",
       " 3697,\n",
       " 3733,\n",
       " 7481,\n",
       " 9414,\n",
       " 2800,\n",
       " 2066,\n",
       " 5024,\n",
       " 3532,\n",
       " 4030,\n",
       " 2235,\n",
       " 2797,\n",
       " 21931,\n",
       " 10539,\n",
       " 2047,\n",
       " 4550,\n",
       " 2917,\n",
       " 10539,\n",
       " 2397,\n",
       " 21446,\n",
       " 11004,\n",
       " 4997,\n",
       " 2917,\n",
       " 2980,\n",
       " 5024,\n",
       " 25192,\n",
       " 7501,\n",
       " 3809,\n",
       " 3697,\n",
       " 7823,\n",
       " 2502,\n",
       " 20868,\n",
       " 4763,\n",
       " 12958,\n",
       " 5024,\n",
       " 3161,\n",
       " 7790,\n",
       " 14480,\n",
       " 5776,\n",
       " 4603,\n",
       " 3893,\n",
       " 12726,\n",
       " 5665,\n",
       " 3491,\n",
       " 6047,\n",
       " 5665,\n",
       " 7098,\n",
       " 2179,\n",
       " 3265,\n",
       " 2214,\n",
       " 2759,\n",
       " 5024,\n",
       " 16021,\n",
       " 4138,\n",
       " 3161,\n",
       " 4603,\n",
       " 3147,\n",
       " 7568,\n",
       " 6555,\n",
       " 2485,\n",
       " 4763,\n",
       " 2214,\n",
       " 13593,\n",
       " 7098,\n",
       " 3835,\n",
       " 5379,\n",
       " 4121,\n",
       " 7568,\n",
       " 4997,\n",
       " 6176,\n",
       " 2917,\n",
       " 5024,\n",
       " 11004,\n",
       " 2397,\n",
       " 3809,\n",
       " 2066,\n",
       " 3893,\n",
       " 4138,\n",
       " 13593,\n",
       " 2800,\n",
       " 4621,\n",
       " 7968,\n",
       " 7568,\n",
       " 5341,\n",
       " 9200,\n",
       " 6450,\n",
       " 4763,\n",
       " 7098,\n",
       " 3733,\n",
       " 4121,\n",
       " 3375,\n",
       " 16021,\n",
       " 25192,\n",
       " 6555,\n",
       " 7591,\n",
       " 7965,\n",
       " 2917,\n",
       " 5600,\n",
       " 5024,\n",
       " 6530,\n",
       " 2800,\n",
       " 2317,\n",
       " 2066,\n",
       " 3375,\n",
       " 3376,\n",
       " 26103,\n",
       " 2917,\n",
       " 7790,\n",
       " 2235,\n",
       " 13135,\n",
       " 2047,\n",
       " 3143,\n",
       " 4030,\n",
       " 3161,\n",
       " 2658,\n",
       " 9191,\n",
       " 3697,\n",
       " 2047,\n",
       " 5665,\n",
       " 3308,\n",
       " 20868,\n",
       " 6832,\n",
       " 2204,\n",
       " 9657,\n",
       " 6047,\n",
       " 3161,\n",
       " 5776,\n",
       " 5410,\n",
       " 2658,\n",
       " 12958,\n",
       " 4010,\n",
       " 13205,\n",
       " 6530,\n",
       " 4010,\n",
       " 3647,\n",
       " 3809,\n",
       " 2182,\n",
       " 5220,\n",
       " 6832,\n",
       " 4010,\n",
       " 3161,\n",
       " 6179,\n",
       " 2304,\n",
       " 22692,\n",
       " 21931,\n",
       " 5600,\n",
       " 7501,\n",
       " 2179,\n",
       " 3375,\n",
       " 3147,\n",
       " 2919,\n",
       " 2844,\n",
       " 7823,\n",
       " 2986,\n",
       " 5024,\n",
       " 7501,\n",
       " 5305,\n",
       " 5024,\n",
       " 17772,\n",
       " 2502,\n",
       " 15716,\n",
       " 5341,\n",
       " 9191,\n",
       " 5710,\n",
       " 2919,\n",
       " 3835,\n",
       " 3265,\n",
       " 4603,\n",
       " 12726,\n",
       " 5024,\n",
       " 2485,\n",
       " 4621,\n",
       " 26103,\n",
       " 4408,\n",
       " 7098,\n",
       " 7591,\n",
       " 2047,\n",
       " 4763,\n",
       " 3265,\n",
       " 2317,\n",
       " 2714,\n",
       " 26103,\n",
       " 3407,\n",
       " 2759,\n",
       " 3491,\n",
       " 2304,\n",
       " 2800,\n",
       " 8796,\n",
       " 2317,\n",
       " 7501,\n",
       " 12511,\n",
       " 4010,\n",
       " 7591,\n",
       " 21446,\n",
       " 4030,\n",
       " 8796,\n",
       " 17145,\n",
       " 3893,\n",
       " 14768,\n",
       " 2986,\n",
       " 3375,\n",
       " 8796,\n",
       " 3893,\n",
       " 17145,\n",
       " 5305,\n",
       " 2714,\n",
       " 3143,\n",
       " 16021,\n",
       " 2502,\n",
       " 2204,\n",
       " 5220,\n",
       " 2844,\n",
       " 21931,\n",
       " 2980,\n",
       " 15716,\n",
       " 26103,\n",
       " 2917,\n",
       " 4603,\n",
       " 3835,\n",
       " 7501,\n",
       " 2214,\n",
       " 6530,\n",
       " 3697,\n",
       " 3697,\n",
       " 4030,\n",
       " 4010,\n",
       " 5220,\n",
       " 14480,\n",
       " 26438,\n",
       " 3491,\n",
       " 3376,\n",
       " 3161,\n",
       " 2917,\n",
       " 12511,\n",
       " 7968,\n",
       " 25192,\n",
       " 5024,\n",
       " 11434,\n",
       " 4326,\n",
       " 6047,\n",
       " 12511,\n",
       " 12726,\n",
       " 26103,\n",
       " 4621,\n",
       " 12511,\n",
       " 7823,\n",
       " 13205,\n",
       " 19194,\n",
       " 21446,\n",
       " 15716,\n",
       " 3733,\n",
       " 17772,\n",
       " 4763,\n",
       " 5665,\n",
       " 12511,\n",
       " 4763,\n",
       " 7968,\n",
       " 3733,\n",
       " 3532,\n",
       " 4763,\n",
       " 14480,\n",
       " 5024,\n",
       " 3161,\n",
       " 4121,\n",
       " 7790,\n",
       " 2047,\n",
       " 2179,\n",
       " 2317,\n",
       " 6179,\n",
       " 6179,\n",
       " 2797,\n",
       " 2304,\n",
       " 3647,\n",
       " 15716,\n",
       " 7591,\n",
       " 4206,\n",
       " 11809,\n",
       " 5305,\n",
       " 6555,\n",
       " 4064,\n",
       " 5600,\n",
       " 8841,\n",
       " 5236,\n",
       " 4763,\n",
       " 26438,\n",
       " 7965,\n",
       " 21446,\n",
       " 26438,\n",
       " 5600,\n",
       " 3563,\n",
       " 10036,\n",
       " 7481,\n",
       " 6450,\n",
       " 6176,\n",
       " 5236,\n",
       " 7098,\n",
       " 4030,\n",
       " 4763,\n",
       " 7098,\n",
       " 15716,\n",
       " 5236,\n",
       " 21877,\n",
       " 7965,\n",
       " 4121,\n",
       " 4658,\n",
       " 12511,\n",
       " 26438,\n",
       " 4795,\n",
       " 7481,\n",
       " 11004,\n",
       " 7501,\n",
       " 3697,\n",
       " 2797,\n",
       " 26438,\n",
       " 13593,\n",
       " 3809,\n",
       " 3819,\n",
       " 6047,\n",
       " 2980,\n",
       " 2917,\n",
       " 11004,\n",
       " 3491,\n",
       " 9841,\n",
       " 5024,\n",
       " 8796,\n",
       " 12726,\n",
       " 10036,\n",
       " 12511,\n",
       " 5236,\n",
       " 3491,\n",
       " 7098,\n",
       " 4603,\n",
       " 4010,\n",
       " 7568,\n",
       " 6450,\n",
       " 4408,\n",
       " 6179,\n",
       " 6179,\n",
       " 6530,\n",
       " 13593,\n",
       " 14768,\n",
       " 2182,\n",
       " 5600,\n",
       " 5710,\n",
       " 3375,\n",
       " 3265,\n",
       " 3407,\n",
       " 5710,\n",
       " 8841,\n",
       " 2917,\n",
       " 9200,\n",
       " 10539,\n",
       " 4064,\n",
       " 2917,\n",
       " 3697,\n",
       " 2047,\n",
       " 7098,\n",
       " 7823,\n",
       " 3376,\n",
       " 6450,\n",
       " 15716,\n",
       " 21931,\n",
       " 2485,\n",
       " 4064,\n",
       " 5341,\n",
       " 3265,\n",
       " 2485,\n",
       " 8796,\n",
       " 12511,\n",
       " 8841,\n",
       " 3308,\n",
       " 12726,\n",
       " 26103,\n",
       " 2397,\n",
       " 2844,\n",
       " 4452,\n",
       " 17145,\n",
       " 6832,\n",
       " 3161,\n",
       " 3407,\n",
       " 25192,\n",
       " 14480,\n",
       " 4763,\n",
       " 6450,\n",
       " 7568,\n",
       " 9200,\n",
       " 4030,\n",
       " 4550,\n",
       " 11809,\n",
       " 12511,\n",
       " 4550,\n",
       " 2317,\n",
       " 7098,\n",
       " 2800,\n",
       " 9841,\n",
       " 21446,\n",
       " 2204,\n",
       " 7965,\n",
       " 5410,\n",
       " 3697,\n",
       " 13593,\n",
       " 22692,\n",
       " 3809,\n",
       " 3893,\n",
       " 2502,\n",
       " 3733,\n",
       " 4030,\n",
       " 3835,\n",
       " 3733,\n",
       " 6832,\n",
       " 11004,\n",
       " 2919,\n",
       " 6047,\n",
       " 4603,\n",
       " 5776,\n",
       " 13135,\n",
       " 10539,\n",
       " 2066,\n",
       " 2919,\n",
       " 2485,\n",
       " 3161,\n",
       " 4621,\n",
       " 2179,\n",
       " 6555,\n",
       " 12511,\n",
       " 22692,\n",
       " 17145,\n",
       " 2214,\n",
       " 4997,\n",
       " 5199,\n",
       " 5515,\n",
       " 9414,\n",
       " 8155,\n",
       " 2236,\n",
       " 4138,\n",
       " 5410,\n",
       " 13135,\n",
       " 6555,\n",
       " 7098,\n",
       " 2981,\n",
       " 2092,\n",
       " 13135,\n",
       " 5665,\n",
       " 10036,\n",
       " 2220,\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741b20ca",
   "metadata": {},
   "source": [
    "### B. Build x,y for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "24272ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73595a48dc984cfd8794922a4c5f0edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112436\n",
      "112432\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "inputs = []\n",
    "max_len = 0\n",
    "for i, sentence in tqdm(enumerate(tokenized_datasets['input_ids'])):\n",
    "    a = np.array(sentence, dtype=int)\n",
    "    y = np.ones_like(a) * -100\n",
    "    y[a == 103] = ys[i]  # cls, token, sep\n",
    "    padded_y = np.zeros(64, dtype=int)\n",
    "    padded_x = np.zeros(64, dtype=int)\n",
    "    padded_y[:len(a)] = y\n",
    "    padded_x[:len(a)] = a\n",
    "    labels.append(padded_y)\n",
    "    inputs.append(padded_x)\n",
    "    \n",
    "l=len(labels)\n",
    "print(l)\n",
    "l = l - (l % 16)\n",
    "print(l)\n",
    "labels = labels[:l]\n",
    "inputs = inputs[:l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "158d775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = datasets.Dataset.from_dict({'labels': labels, 'input_ids': inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3af437ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = lm_datasets.shuffle().train_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460f444b",
   "metadata": {},
   "source": [
    "### (tokenizer check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5372998f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7a26c541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b340ce3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] howard is often [MASK]. we want a fearful person to fill the same need. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets['train'][4]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c71123a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2010, 2025, 5649, 12102, 2000, 2022, 15241, 103, 1012, 2002, 4122, 1037, 9657, 2028, 2000, 6039, 1996, 2168, 2342]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, 11004, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[101, 1000, 4326, 1000, 2038, 2019, 4941, 3574, 2000, 1000, 103, 1000, 1010, 2061, 2065, 1037, 3274, 2003, 5220, 2009]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5220, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[101, 6832, 11265, 5867, 2015, 103, 1010, 2947, 2065, 1037, 3797, 2003, 18439, 2009, 2196, 2003, 6832, 1012, 102, 0]\n",
      "[-100, -100, -100, -100, -100, 18439, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 0]\n",
      "[101, 2009, 12102, 2000, 2022, 2172, 2205, 4064, 1012, 2016, 2359, 1037, 103, 8000, 2004, 2019, 4522, 1012, 102, 0]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2440, -100, -100, -100, -100, -100, -100, 0]\n",
      "[101, 4922, 2003, 2411, 103, 1012, 2057, 2215, 1037, 19725, 2711, 2000, 6039, 1996, 2168, 2342, 1012, 102, 0, 0]\n",
      "[-100, -100, -100, -100, 26103, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(lm_datasets['train'][i]['input_ids'][:20])\n",
    "    print(lm_datasets['train'][i]['labels'][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c4a6c",
   "metadata": {},
   "source": [
    "### build collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b772d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1b1ccc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    f\"model-mlm-generated-text\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "36c20134",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f84770",
   "metadata": {},
   "source": [
    "### This trainer stores data masked - dataloader does nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b1415101",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = trainer.get_train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fc377c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  5023,  -100,  -100,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          6555,  -100,  -100,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [ -100,  5199,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  3625,  -100,  -100,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 10036,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100, 11809,  -100,  -100,  -100,  -100,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  4795,  -100,  -100,  -100,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [ -100,  2066,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]]), 'input_ids': tensor([[  101,  2043,  1037,  2518,  2003,  3491,  1010,  2009,  2097,  2025,\n",
      "          2022,   103,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2065,  5032,  2003,  2025,  4603,  1010,  2016,  2442,  2022,\n",
      "           103,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,   103, 27353,  1996,  7901,  1997,  9191,  1010,  2061,  2057,\n",
      "          2113,  2008,  2065,  1037,  2518,  2003,  5199,  3593,  2009,  3685,\n",
      "          2022,  9191,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2065,  4907,  2003,  2025, 20868,  6072, 26029, 19307,  1010,\n",
      "          2002,  2097,  5525,  2022,   103,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  6330,  2003,  2025, 10036,  1012,  2057,  2215,  1037,   103,\n",
      "          2711,  2000,  6039,  1996,  2168,  2342,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2008,  3797, 12102,  2000,  2022, 15241,  6179,  1012,  1045,\n",
      "          2097,  2215,  1037,   103,  3797,  2612,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  9399, 12102,  2000,  2022, 15241,  3647,  1012,  2672,  2057,\n",
      "          2064,  2424,  1037,   103,  2711,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,   103, 11265,  5867,  2015,  2367,  1010,  2029, 12748,  2065,\n",
      "          1037,  2518,  2003,  2367,  2009,  2196,  2003,  2066,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "for s in dl:\n",
    "    print(s)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "341b75f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_sample(i):\n",
    "    print(tokenizer.decode(s['input_ids'][i]))\n",
    "    print(s['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ab5a721e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] when a thing is shown, it will not be [MASK]. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5023,\n",
      "        -100, -100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "view_sample(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e1e0b",
   "metadata": {},
   "source": [
    "# train / evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e638b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 84324\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 31623\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='851' max='31623' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  851/31623 01:08 < 41:08, 12.47 it/s, Epoch 0.08/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model-mlm-generated-text/checkpoint-500\n",
      "Configuration saved in model-mlm-generated-text/checkpoint-500/config.json\n",
      "Model weights saved in model-mlm-generated-text/checkpoint-500/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04c28f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraForMaskedLM(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (generator_predictions): ElectraGeneratorPredictions(\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (dense): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       "  (generator_lm_head): Linear(in_features=128, out_features=30522, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4a25e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pipeline('fill-mask', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "036a0b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'huggingface is creating a. that the community uses to solve nlp tasks.',\n",
       "  'score': 0.0022701651323586702,\n",
       "  'token': 1012,\n",
       "  'token_str': '.'},\n",
       " {'sequence': 'huggingface is creating a a that the community uses to solve nlp tasks.',\n",
       "  'score': 0.0019472201820462942,\n",
       "  'token': 1037,\n",
       "  'token_str': 'a'},\n",
       " {'sequence': 'huggingface is creating a not that the community uses to solve nlp tasks.',\n",
       "  'score': 0.0016775872791185975,\n",
       "  'token': 2025,\n",
       "  'token_str': 'not'},\n",
       " {'sequence': 'huggingface is creating a that that the community uses to solve nlp tasks.',\n",
       "  'score': 0.0015871956711634994,\n",
       "  'token': 2008,\n",
       "  'token_str': 'that'},\n",
       " {'sequence': 'huggingface is creating a i that the community uses to solve nlp tasks.',\n",
       "  'score': 0.0015854841331019998,\n",
       "  'token': 1045,\n",
       "  'token_str': 'i'}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p(f\"HuggingFace is creating a {tokenizer.mask_token} that the community uses to solve NLP tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1872ab61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (a5)",
   "language": "python",
   "name": "pycharm-f95a7fa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
