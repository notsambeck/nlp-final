{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "275c34b1",
   "metadata": {},
   "source": [
    "# expanding the provided SQuAD training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "244d4dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, \\\n",
    "    AutoModelForQuestionAnswering, Trainer, TrainingArguments, HfArgumentParser, pipeline\n",
    "from helpers import prepare_dataset_nli, prepare_train_dataset_qa, \\\n",
    "    prepare_validation_dataset_qa, QuestionAnsweringTrainer, compute_accuracy\n",
    "import os\n",
    "import json\n",
    "\n",
    "NUM_PREPROCESSING_WORKERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "694fcc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL='google/electra-small-discriminator'\n",
    "TASK='qa'\n",
    "DATASET='squad'\n",
    "MAX_LENGTH=128\n",
    "TRAIN_MAX_SAMPLES=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052cd2e4",
   "metadata": {},
   "source": [
    "### compare new Checklist-based dataset with Squad\n",
    "\n",
    "(you can make a basic Checklist dataset using the make_checklist_dataset notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d748050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.Dataset.from_file('./new_dataset/dataset.arrow')\n",
    "dataset = dataset.shuffle()\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4df3121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('id',\n",
       "              ['56e7a89837bdd419002c42e0',\n",
       "               '3fb01864d8757d4f',\n",
       "               '5c519404bb373f50',\n",
       "               '573416fcd058e614000b6917',\n",
       "               '56e0a85e7aa994140058e69f',\n",
       "               '571aa18a4faf5e1900b8ab68',\n",
       "               '571df47eb64a571400c71e24',\n",
       "               '5727af18ff5b5019007d9284',\n",
       "               '570e69cb0dc6ce1900205046',\n",
       "               '5735e8d3012e2f140011a0d5',\n",
       "               '5711252ab654c5140001fbc1',\n",
       "               '56f88690a6d7ea1400e17723',\n",
       "               '571a7fbe4faf5e1900b8a9eb',\n",
       "               '5726f906f1498d1400e8f175',\n",
       "               '57282fdf3acd2414000df694',\n",
       "               '56df6a8d5ca0a614008f99d9',\n",
       "               'x31389474e271a6a1',\n",
       "               '572f8332a23a5019007fc6bb',\n",
       "               '5728fb8a6aef05140015493f',\n",
       "               '56bf725c3aeaaa14008c9647',\n",
       "               '5723eae00dadf01500fa1fa1',\n",
       "               '5727fbf84b864d1900164158',\n",
       "               '5730146f947a6a140053d07e',\n",
       "               '572b7df5f75d5e190021fe0c',\n",
       "               '5727478cf1498d1400e8f59e',\n",
       "               '419c3e62b0701074',\n",
       "               '9d1416f36b055b5',\n",
       "               '572fa95304bcaa1900d76b71',\n",
       "               '56de7066cffd8e1900b4b8cc',\n",
       "               '573258d2e17f3d14004228d1']),\n",
       "             ('title',\n",
       "              ['Daylight_saving_time',\n",
       "               'who_is_more_x',\n",
       "               'who_is_more_x',\n",
       "               'Richmond,_Virginia',\n",
       "               'Russian_Soviet_Federative_Socialist_Republic',\n",
       "               'Umayyad_Caliphate',\n",
       "               'Multiracial_American',\n",
       "               'Switzerland',\n",
       "               'Order_of_the_British_Empire',\n",
       "               'Hunting',\n",
       "               'Nintendo_Entertainment_System',\n",
       "               'Martin_Luther',\n",
       "               'Ashkenazi_Jews',\n",
       "               'Madonna_(entertainer)',\n",
       "               'Annelid',\n",
       "               'Christian',\n",
       "               'who_is_less_x',\n",
       "               'Post-punk',\n",
       "               'Samurai',\n",
       "               'Beyoncé',\n",
       "               'Queen_Victoria',\n",
       "               'Gamal_Abdel_Nasser',\n",
       "               'Georgian_architecture',\n",
       "               'Zinc',\n",
       "               'Private_school',\n",
       "               'who_is_more_x',\n",
       "               'who_is_less_x',\n",
       "               'Greeks',\n",
       "               'Heresy',\n",
       "               'Jehovah%27s_Witnesses']),\n",
       "             ('context',\n",
       "              ['Microsoft Windows keeps the system real-time clock in local time. This causes several problems, including compatibility when multi booting with operating systems that set the clock to UTC, and double-adjusting the clock when multi booting different Windows versions, such as with a rescue boot disk. This approach is a problem even in Windows-only systems: there is no support for per-user timezone settings, only a single system-wide setting. In 2008 Microsoft hinted that future versions of Windows will partially support a Windows registry entry RealTimeIsUniversal that had been introduced many years earlier, when Windows NT supported RISC machines with UTC clocks, but had not been maintained. Since then at least two fixes related to this feature have been published by Microsoft.',\n",
       "               'Andrew is funny, but Lawrence is more funny.',\n",
       "               'Adam is individual, but Jonathan is more individual.',\n",
       "               'Richmond is located at the fall line of the James River, 44 miles (71 km) west of Williamsburg, 66 miles (106 km) east of Charlottesville, and 98 miles (158 km) south of Washington, D.C. Surrounded by Henrico and Chesterfield counties, the city is located at the intersections of Interstate 95 and Interstate 64, and encircled by Interstate 295 and Virginia State Route 288. Major suburbs include Midlothian to the southwest, Glen Allen to the north and west, Short Pump to the west and Mechanicsville to the northeast.',\n",
       "               'The Government was known officially as the Council of People\\'s Commissars (1917–1946), Council of Ministers (1946–1978) and Council of Ministers–Government (1978–1991). The first government was headed by Vladimir Lenin as \"Chairman of the Council of People\\'s Commissars of the Russian SFSR\" and the last by Boris Yeltsin as both head of government and head of state under the title \"President\".',\n",
       "               'Umar is honored for his attempt to resolve the fiscal problems attendant upon conversion to Islam. During the Umayyad period, the majority of people living within the caliphate were not Muslim, but Christian, Jewish, Zoroastrian, or members of other small groups. These religious communities were not forced to convert to Islam, but were subject to a tax (jizyah) which was not imposed upon Muslims. This situation may actually have made widespread conversion to Islam undesirable from the point of view of state revenue, and there are reports that provincial governors actively discouraged such conversions. It is not clear how Umar attempted to resolve this situation, but the sources portray him as having insisted on like treatment of Arab and non-Arab (mawali) Muslims, and on the removal of obstacles to the conversion of non-Arabs to Islam.',\n",
       "               'Chinese men entered the United States as laborers, primarily on the West Coast and in western territories. Following the Reconstruction era, as blacks set up independent farms, white planters imported Chinese laborers to satisfy their need for labor. In 1882, the Chinese Exclusion Act was passed, and Chinese workers who chose to stay in the U.S. were unable to have their wives join them. In the South, some Chinese married into the black and mulatto communities, as generally discrimination meant they did not take white spouses. They rapidly left working as laborers, and set up groceries in small towns throughout the South. They worked to get their children educated and socially mobile.',\n",
       "               \"Swiss private-public managed road network is funded by road tolls and vehicle taxes. The Swiss autobahn/autoroute system requires the purchase of a vignette (toll sticker)—which costs 40 Swiss francs—for one calendar year in order to use its roadways, for both passenger cars and trucks. The Swiss autobahn/autoroute network has a total length of 1,638 km (1,018 mi) (as of 2000) and has, by an area of 41,290 km2 (15,940 sq mi), also one of the highest motorway densities in the world. Zürich Airport is Switzerland's largest international flight gateway, which handled 22.8 million passengers in 2012. The other international airports are Geneva Airport (13.9 million passengers in 2012), EuroAirport Basel-Mulhouse-Freiburg which is located in France, Bern Airport, Lugano Airport, St. Gallen-Altenrhein Airport and Sion Airport. Swiss International Air Lines is the flag carrier of Switzerland. Its main hub is Zürich.\",\n",
       "               'At the foundation of the Order, the \"Medal of the Order of the British Empire\" was instituted, to serve as a lower award granting recipients affiliation but not membership. In 1922, this was renamed the \"British Empire Medal\". It stopped being awarded by the United Kingdom as part of the 1993 reforms to the honours system, but was again awarded beginning in 2012, starting with 293 BEMs awarded for the Queen\\'s Diamond Jubilee. In addition, the BEM is awarded by the Cook Islands and by some other Commonwealth nations. In 2004, a report entitled \"A Matter of Honour: Reforming Our Honours System\" by a Commons committee recommended to phase out the Order of the British Empire, as its title was \"now considered to be unacceptable, being thought to embody values that are no longer shared by many of the country’s population\".',\n",
       "               'Archaeologist Louis Binford criticised the idea that early hominids and early humans were hunters. On the basis of the analysis of the skeletal remains of the consumed animals, he concluded that hominids and early humans were mostly scavengers, not hunters, and this idea is popular among some archaeologists and paleoanthropologists. Robert Blumenschine proposed the idea of confrontational scavenging, which involves challenging and scaring off other predators after they have made a kill, which he suggests could have been the leading method of obtaining protein-rich meat by early humans.',\n",
       "               'The NES was released after the \"video game crash\" of the early 1980s, whereupon many retailers and adults had regarded electronic games as being merely a passing fad, and many believed at first that the NES was another fad. Before the NES/Famicom, Nintendo was known as a moderately successful Japanese toy and playing card manufacturer, and the popularity of the NES/Famicom helped the company grow into an internationally recognized name almost synonymous with video games as Atari had been during the 2600 era and set the stage for Japanese dominance of the video game industry. With the NES, Nintendo also changed the relationship of console manufacturers and third-party software developers by restricting developers from publishing and distributing software without licensed approval. This led to higher quality software titles, which helped to change the attitude of a public that had grown weary from poorly produced titles for other game systems of the day.',\n",
       "               'Luther\\'s Commentary on Genesis contains a passage which concludes that \"the soul does not sleep (anima non sic dormit), but wakes (sed vigilat) and experiences visions\". Francis Blackburne in 1765 argued that John Jortin misread this and other passages from Luther, while Gottfried Fritschel pointed out in 1867 that it actually refers to the soul of a man \"in this life\" (homo enim in hac vita) tired from his daily labour (defatigus diurno labore) who at night enters his bedchamber (sub noctem intrat in cubiculum suum) and whose sleep is interrupted by dreams.',\n",
       "               'Although the Jewish people in general were present across a wide geographical area as described, genetic research done by Gil Atzmon of the Longevity Genes Project at Albert Einstein College of Medicine suggests \"that Ashkenazim branched off from other Jews around the time of the destruction of the First Temple, 2,500 years ago ... flourished during the Roman Empire but then went through a \\'severe bottleneck\\' as they dispersed, reducing a population of several million to just 400 families who left Northern Italy around the year 1000 for Central and eventually Eastern Europe.\"',\n",
       "               'During her childhood, Madonna was inspired by actors, later saying, \"I loved Carole Lombard and Judy Holliday and Marilyn Monroe. They were all incredibly funny ... and I saw myself in them ... my girlishness, my knowingness and my innocence.\" Her \"Material Girl\" music video recreated Monroe\\'s look in the song \"Diamonds Are a Girl\\'s Best Friend\", from the film Gentlemen Prefer Blondes (1953). She studied the screwball comedies of the 1930s, particularly those of Lombard, in preparation for the film Who\\'s That Girl. The video for \"Express Yourself\" (1989) was inspired by Fritz Lang\\'s silent film Metropolis (1927). The video for \"Vogue\" recreated the style of Hollywood glamour photographs, in particular those by Horst P. Horst, and imitated the poses of Marlene Dietrich, Carole Lombard, and Rita Hayworth, while the lyrics referred to many of the stars who had inspired her, including Bette Davis, described by Madonna as an idol. However, Madonna\\'s film career has been largely received negatively by the film critic community. Stephanie Zacharek, critic for Time magazine, stated that, \"[Madonna] seems wooden and unnatural as an actress, and it\\'s tough to watch, because she\\'s clearly trying her damnedest.\" According to biographer Andrew Morton, \"Madonna puts a brave face on the criticism, but privately she is deeply hurt.\" After the box office bomb Swept Away (2002), Madonna vowed that she would never again act in a film, hoping her repertoire as a bad actress would never be discussed again.',\n",
       "               'Most mature clitellates (the group that includes earthworms and leeches) are full hermaphrodites, although in a few leech species younger adults function as males and become female at maturity. All have well-developed gonads, and all copulate. Earthworms store their partners\\' sperm in spermathecae (\"sperm stores\") and then the clitellum produces a cocoon that collects ova from the ovaries and then sperm from the spermathecae. Fertilization and development of earthworm eggs takes place in the cocoon. Leeches\\' eggs are fertilized in the ovaries, and then transferred to the cocoon. In all clitellates the cocoon also either produces yolk when the eggs are fertilized or nutrients while they are developing. All clitellates hatch as miniature adults rather than larvae.',\n",
       "               'Another Arabic word sometimes used for Christians, particularly in a political context, is Ṣalībī (صليبي \"Crusader\") from ṣalīb (صليب \"cross\") which refers to Crusaders and has negative connotations. However, Salibi is a modern term; historically, Muslim writers described European Christian Crusaders as al-Faranj or Alfranj (الفرنج) and Firinjīyah (الفرنجيّة) in Arabic\" This word comes from the Franks and can be seen in the Arab history text Al-Kamil fi al-Tarikh by Ali ibn al-Athir.',\n",
       "               'Susan is interactive, but Julia is more interactive.',\n",
       "               'As the initial punk movement dwindled, vibrant new scenes began to coalesce out of a variety of bands pursuing experimental sounds and wider conceptual territory in their work. Many of these artists drew on backgrounds in art and viewed their music as invested in particular political or aesthetic agendas. British music publications such as the NME and Sounds developed an influential part in this nascent post-punk culture, with writers like Jon Savage, Paul Morley and Ian Penman developing a dense (and often playful) style of criticism that drew on critical theory, radical politics and an eclectic variety of other sources.',\n",
       "               \"In the same war, the Prussian Edward Schnell served the Aizu domain as a military instructor and procurer of weapons. He was granted the Japanese name Hiramatsu Buhei (平松武兵衛), which inverted the characters of the daimyo's name Matsudaira. Hiramatsu (Schnell) was given the right to wear swords, as well as a residence in the castle town of Wakamatsu, a Japanese wife, and retainers. In many contemporary references, he is portrayed wearing a Japanese kimono, overcoat, and swords, with Western riding trousers and boots.\",\n",
       "               'A self-described \"modern-day feminist\", Beyoncé creates songs that are often characterized by themes of love, relationships, and monogamy, as well as female sexuality and empowerment. On stage, her dynamic, highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music. Throughout a career spanning 19 years, she has sold over 118 million records as a solo artist, and a further 60 million with Destiny\\'s Child, making her one of the best-selling music artists of all time. She has won 20 Grammy Awards and is the most nominated woman in the award\\'s history. The Recording Industry Association of America recognized her as the Top Certified Artist in America during the 2000s decade. In 2009, Billboard named her the Top Radio Songs Artist of the Decade, the Top Female Artist of the 2000s and their Artist of the Millennium in 2011. Time listed her among the 100 most influential people in the world in 2013 and 2014. Forbes magazine also listed her as the most powerful female musician of 2015.',\n",
       "               'Victoria\\'s self-imposed isolation from the public diminished the popularity of the monarchy, and encouraged the growth of the republican movement. She did undertake her official government duties, yet chose to remain secluded in her royal residences—Windsor Castle, Osborne House, and the private estate in Scotland that she and Albert had acquired in 1847, Balmoral Castle. In March 1864, a protester stuck a notice on the railings of Buckingham Palace that announced \"these commanding premises to be let or sold in consequence of the late occupant\\'s declining business\". Her uncle Leopold wrote to her advising her to appear in public. She agreed to visit the gardens of the Royal Horticultural Society at Kensington and take a drive through London in an open carriage.',\n",
       "               'In January 1953, Nasser overcame opposition from Naguib and banned all political parties, creating a one-party system under the Liberation Rally, a loosely structured movement whose chief task was to organize pro-RCC rallies and lectures, with Nasser its secretary-general. Despite the dissolution order, Nasser was the only RCC member who still favored holding parliamentary elections, according to his fellow officer Abdel Latif Boghdadi. Although outvoted, he still advocated holding elections by 1956. In March 1953, Nasser led the Egyptian delegation negotiating a British withdrawal from the Suez Canal.',\n",
       "               \"The styles that resulted fall within several categories. In the mainstream of Georgian style were both Palladian architecture— and its whimsical alternatives, Gothic and Chinoiserie, which were the English-speaking world's equivalent of European Rococo. From the mid-1760s a range of Neoclassical modes were fashionable, associated with the British architects Robert Adam, James Gibbs, Sir William Chambers, James Wyatt, George Dance the Younger, Henry Holland and Sir John Soane. John Nash was one of the most prolific architects of the late Georgian era known as The Regency style, he was responsible for designing large areas of London. Greek Revival architecture was added to the repertory, beginning around 1750, but increasing in popularity after 1800. Leading exponents were William Wilkins and Robert Smirke.\",\n",
       "               'Zinc chloride is often added to lumber as a fire retardant and can be used as a wood preservative. It is also used to make other chemicals. Zinc methyl (Zn(CH3)\\n2) is used in a number of organic syntheses. Zinc sulfide (ZnS) is used in luminescent pigments such as on the hands of clocks, X-ray and television screens, and luminous paints. Crystals of ZnS are used in lasers that operate in the mid-infrared part of the spectrum. Zinc sulfate is a chemical in dyes and pigments. Zinc pyrithione is used in antifouling paints.',\n",
       "               'The secondary level includes schools offering years 7 through 12 (year twelve is known as lower sixth) and year 13 (upper sixth). This category includes university-preparatory schools or \"prep schools\", boarding schools and day schools. Tuition at private secondary schools varies from school to school and depends on many factors, including the location of the school, the willingness of parents to pay, peer tuitions and the school\\'s financial endowment. High tuition, schools claim, is used to pay higher salaries for the best teachers and also used to provide enriched learning environments, including a low student to teacher ratio, small class sizes and services, such as libraries, science laboratories and computers. Some private schools are boarding schools and many military academies are privately owned or operated as well.',\n",
       "               'Edward is engaging, but Donald is more engaging.',\n",
       "               'Diane is individual, but Donna is more individual.',\n",
       "               'The evolution of Proto-Greek should be considered within the context of an early Paleo-Balkan sprachbund that makes it difficult to delineate exact boundaries between individual languages. The characteristically Greek representation of word-initial laryngeals by prothetic vowels is shared, for one, by the Armenian language, which also seems to share some other phonological and morphological peculiarities of Greek; this has led some linguists to propose a hypothetical closer relationship between Greek and Armenian, although evidence remains scant.',\n",
       "               'The use of the word \"heresy\" was given wide currency by Irenaeus in his 2nd century tract Contra Haereses (Against Heresies) to describe and discredit his opponents during the early centuries of the Christian community.[citation needed] He described the community\\'s beliefs and doctrines as orthodox (from ὀρθός, orthos \"straight\" + δόξα, doxa \"belief\") and the Gnostics\\' teachings as heretical.[citation needed] He also pointed out the concept of apostolic succession to support his arguments.',\n",
       "               'In the United States, their persistent legal challenges prompted a series of state and federal court rulings that reinforced judicial protections for civil liberties. Among the rights strengthened by Witness court victories in the United States are the protection of religious conduct from federal and state interference, the right to abstain from patriotic rituals and military service, the right of patients to refuse medical treatment, and the right to engage in public discourse. Similar cases in their favor have been heard in Canada.']),\n",
       "             ('question',\n",
       "              ['Since 2008, at least how many times has Microsoft released fixes for the RealTimeIsUniversal feature?',\n",
       "               'Who is the most funny?',\n",
       "               'Who is the most individual?',\n",
       "               'How many kilometers west of Charlottesville is Richmond?',\n",
       "               'Who led the final government of the RSFSR?',\n",
       "               'Along with Christians and Jews, what was a major non-Muslim religious group under the Umayyads?',\n",
       "               'Where did most Chinese men enter the US?',\n",
       "               'How many passengers used the Zurich airport in 2012?',\n",
       "               'In what year was the Medal of the Order of the British Empire established?',\n",
       "               \"Louis Binford's idea is popular among whom?\",\n",
       "               'Nintendo produced toys and what other item before its game system?',\n",
       "               \"Where did Luther say that the soul doesn't sleep, but rather has visions?\",\n",
       "               'The destruction of the First Temple was how many years ago?',\n",
       "               \"Madonna's Material Girl recreated whose look?\",\n",
       "               'What type of sex are most earthworms?',\n",
       "               'What is the Arabic term when referring to Christians in a political sense?',\n",
       "               'Who is the least interactive?',\n",
       "               'What was a common theme to post-punk music?',\n",
       "               \"What was Edward Schnell's Japanese name?\",\n",
       "               'How many records has Beyonce sold in her 19 year career?',\n",
       "               'What year did a protester attach a note to the door at Buckingham Palace saying it was to be sold?',\n",
       "               \"What was Nasser's first title in the Liberation Rally?\",\n",
       "               'Around what year was the Greek Revival added to the repertory?',\n",
       "               'What kind of lasers are crystals of zinc suflde used in?',\n",
       "               'Who receives higher salaries at private schools that charge higher tuition?',\n",
       "               'Who is the most engaging?',\n",
       "               'Who is the least individual?',\n",
       "               'What other language has this same trait ?',\n",
       "               'Who gave more exposure to the term heresy when attempting to descredit opponents during the early centuries of Christianity?',\n",
       "               \"What did Jehovah's Witnesses persistent legal challenges result in, in the United States?\"]),\n",
       "             ('answers',\n",
       "              [{'answer_start': [720], 'text': ['two']},\n",
       "               {'answer_start': [21], 'text': ['Lawrence']},\n",
       "               {'answer_start': [24], 'text': ['Jonathan']},\n",
       "               {'answer_start': [106], 'text': ['106']},\n",
       "               {'answer_start': [307], 'text': ['Boris Yeltsin']},\n",
       "               {'answer_start': [217], 'text': ['Zoroastrian']},\n",
       "               {'answer_start': [51],\n",
       "                'text': ['primarily on the West Coast and in western territories']},\n",
       "               {'answer_start': [571], 'text': ['22.8 million']},\n",
       "               {'answer_start': [176], 'text': ['1922']},\n",
       "               {'answer_start': [294],\n",
       "                'text': ['archaeologists and paleoanthropologists']},\n",
       "               {'answer_start': [311], 'text': ['playing card']},\n",
       "               {'answer_start': [9, 9, 9],\n",
       "                'text': ['Commentary on Genesis',\n",
       "                 'Commentary on Genesis',\n",
       "                 'Commentary on Genesis']},\n",
       "               {'answer_start': [314], 'text': ['2,500 years ago']},\n",
       "               {'answer_start': [286], 'text': [\"Monroe's\"]},\n",
       "               {'answer_start': [82], 'text': ['hermaphrodites']},\n",
       "               {'answer_start': [91], 'text': ['Ṣalībī']},\n",
       "               {'answer_start': [0], 'text': ['Susan']},\n",
       "               {'answer_start': [275],\n",
       "                'text': ['political or aesthetic agendas']},\n",
       "               {'answer_start': [151], 'text': ['Hiramatsu Buhei']},\n",
       "               {'answer_start': [393], 'text': ['118 million']},\n",
       "               {'answer_start': [384], 'text': ['1864']},\n",
       "               {'answer_start': [255], 'text': ['secretary-general']},\n",
       "               {'answer_start': [705], 'text': ['around 1750']},\n",
       "               {'answer_start': [395], 'text': ['mid-infrared']},\n",
       "               {'answer_start': [530, 530, 525],\n",
       "                'text': ['teachers', 'teachers', 'best teachers']},\n",
       "               {'answer_start': [24], 'text': ['Donald']},\n",
       "               {'answer_start': [0], 'text': ['Diane']},\n",
       "               {'answer_start': [236],\n",
       "                'text': ['word-initial laryngeals by prothetic vowels is shared, for one, by the Armenian language']},\n",
       "               {'answer_start': [56], 'text': ['Irenaeus']},\n",
       "               {'answer_start': [114],\n",
       "                'text': ['reinforced judicial protections for civil liberties']}])])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89e56381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/sambeck/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "id\n",
      "['5733be284776f41900661182', '5733be284776f4190066117f']\n",
      "['56e7a89837bdd419002c42e0', '3fb01864d8757d4f']\n",
      "\n",
      "title\n",
      "['University_of_Notre_Dame', 'University_of_Notre_Dame']\n",
      "['Daylight_saving_time', 'who_is_more_x']\n",
      "\n",
      "context\n",
      "['Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.']\n",
      "['Microsoft Windows keeps the system real-time clock in local time. This causes several problems, including compatibility when multi booting with operating systems that set the clock to UTC, and double-adjusting the clock when multi booting different Windows versions, such as with a rescue boot disk. This approach is a problem even in Windows-only systems: there is no support for per-user timezone settings, only a single system-wide setting. In 2008 Microsoft hinted that future versions of Windows will partially support a Windows registry entry RealTimeIsUniversal that had been introduced many years earlier, when Windows NT supported RISC machines with UTC clocks, but had not been maintained. Since then at least two fixes related to this feature have been published by Microsoft.', 'Andrew is funny, but Lawrence is more funny.']\n",
      "\n",
      "question\n",
      "['To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'What is in front of the Notre Dame Main Building?']\n",
      "['Since 2008, at least how many times has Microsoft released fixes for the RealTimeIsUniversal feature?', 'Who is the most funny?']\n",
      "\n",
      "answers\n",
      "[{'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}, {'text': ['a copper statue of Christ'], 'answer_start': [188]}]\n",
      "[{'answer_start': [720], 'text': ['two']}, {'answer_start': [21], 'text': ['Lawrence']}]\n"
     ]
    }
   ],
   "source": [
    "sqd = datasets.load_dataset('squad')\n",
    "for col in ['id', 'title', 'context', 'question', 'answers']:\n",
    "    print()\n",
    "    print(col)\n",
    "    print(sqd['train'][col][0:2])\n",
    "    print(dataset['train'][col][0:2])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f65e6",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4833e5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_class = AutoModelForQuestionAnswering\n",
    "# Initialize the model and tokenizer from the specified pretrained model/checkpoint\n",
    "model = model_class.from_pretrained(MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b05a1f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model pipeline (includes tokenization)\n",
    "mp = pipeline(\"question-answering\", tokenizer=tokenizer, model=model, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bfb0516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.030282270163297653, 'start': 0, 'end': 5, 'answer': 'There'}\n"
     ]
    }
   ],
   "source": [
    "pred = mp(question=\"Which color is the dog?\", context=\"There is a black dog.\", truncation=True, )\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ebfe4e",
   "metadata": {},
   "source": [
    "### Set up dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6bcccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_featurized = None\n",
    "eval_dataset_featurized = None\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a32481ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featurize train...\n",
      "featurize test...\n"
     ]
    }
   ],
   "source": [
    "prepare_train_dataset = lambda exs: prepare_train_dataset_qa(exs, tokenizer)\n",
    "prepare_eval_dataset = lambda exs: prepare_validation_dataset_qa(exs, tokenizer)\n",
    "\n",
    "if TRAIN_MAX_SAMPLES:\n",
    "    train_dataset = train_dataset.select(range(TRAIN_MAX_SAMPLES))\n",
    "\n",
    "print('featurize train...')\n",
    "train_dataset_featurized = train_dataset.map(\n",
    "    prepare_train_dataset,\n",
    "    batched=True,\n",
    "    num_proc=NUM_PREPROCESSING_WORKERS,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "print('featurize test...')\n",
    "eval_dataset_featurized = eval_dataset.map(\n",
    "    prepare_eval_dataset,\n",
    "    batched=True,\n",
    "    num_proc=NUM_PREPROCESSING_WORKERS,\n",
    "    remove_columns=eval_dataset.column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "224fc06e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d7563a9989b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meval_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# If you want to use custom metrics, you should define your own \"compute_metrics\" function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# For an example of a valid compute_metrics function, see compute_accuracy in helpers.py.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcompute_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer_class = Trainer\n",
    "eval_kwargs = {}\n",
    "# If you want to use custom metrics, you should define your own \"compute_metrics\" function.\n",
    "# For an example of a valid compute_metrics function, see compute_accuracy in helpers.py.\n",
    "compute_metrics = None\n",
    "# For QA, we need to use a tweaked version of the Trainer (defined in helpers.py)\n",
    "# to enable the question-answering specific evaluation metrics\n",
    "trainer_class = QuestionAnsweringTrainer\n",
    "eval_kwargs['eval_examples'] = eval_dataset\n",
    "metric = datasets.load_metric('squad')\n",
    "compute_metrics = lambda eval_preds: metric.compute(\n",
    "    predictions=eval_preds.predictions, references=eval_preds.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2628dccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function wraps the compute_metrics function, storing the model's predictions\n",
    "# so that they can be dumped along with the computed metrics\n",
    "eval_predictions = None\n",
    "def compute_metrics_and_store_predictions(eval_preds):\n",
    "    global eval_predictions\n",
    "    eval_predictions = eval_preds\n",
    "    return compute_metrics(eval_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a738b3f4",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d67abe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer object with the specified arguments and the model and dataset we loaded above\n",
    "trainer = trainer_class(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset_featurized,\n",
    "    eval_dataset=eval_dataset_featurized,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_and_store_predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a30b27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 96522\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 36198\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32001' max='36198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32001/36198 2:39:55 < 20:58, 3.33 it/s, Epoch 2.65/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.062600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.119100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.692700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.434000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.359700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.309100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.318300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.285800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.236100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.230400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.166100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.174800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.173600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.140100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.143800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.142100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.119700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.133300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.097700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.097900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.939800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.923000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.902000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.948600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.939300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.893100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.902300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.925600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.884900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.932800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.893400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.867000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.894900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.907800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.867200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.900900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.904500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.934400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.914800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.929200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.861100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.852300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.783000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.714400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.740300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.765600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.736700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.733500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.753600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.696900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.718300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.717100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.766900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.691200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.724500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.721800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.715100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to tmp_trainer/checkpoint-500\n",
      "Configuration saved in tmp_trainer/checkpoint-500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-1000\n",
      "Configuration saved in tmp_trainer/checkpoint-1000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-1500\n",
      "Configuration saved in tmp_trainer/checkpoint-1500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-2000\n",
      "Configuration saved in tmp_trainer/checkpoint-2000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-2500\n",
      "Configuration saved in tmp_trainer/checkpoint-2500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-3000\n",
      "Configuration saved in tmp_trainer/checkpoint-3000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-3500\n",
      "Configuration saved in tmp_trainer/checkpoint-3500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-4000\n",
      "Configuration saved in tmp_trainer/checkpoint-4000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-4500\n",
      "Configuration saved in tmp_trainer/checkpoint-4500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-5000\n",
      "Configuration saved in tmp_trainer/checkpoint-5000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-5500\n",
      "Configuration saved in tmp_trainer/checkpoint-5500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-6000\n",
      "Configuration saved in tmp_trainer/checkpoint-6000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-6000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-6500\n",
      "Configuration saved in tmp_trainer/checkpoint-6500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-6500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-7000\n",
      "Configuration saved in tmp_trainer/checkpoint-7000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-7000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-7500\n",
      "Configuration saved in tmp_trainer/checkpoint-7500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-7500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-8000\n",
      "Configuration saved in tmp_trainer/checkpoint-8000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-8000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-8500\n",
      "Configuration saved in tmp_trainer/checkpoint-8500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-8500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-9000\n",
      "Configuration saved in tmp_trainer/checkpoint-9000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-9000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-9500\n",
      "Configuration saved in tmp_trainer/checkpoint-9500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-9500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-10000\n",
      "Configuration saved in tmp_trainer/checkpoint-10000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-10000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-10500\n",
      "Configuration saved in tmp_trainer/checkpoint-10500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-10500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-11000\n",
      "Configuration saved in tmp_trainer/checkpoint-11000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-11000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-11500\n",
      "Configuration saved in tmp_trainer/checkpoint-11500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-11500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-12000\n",
      "Configuration saved in tmp_trainer/checkpoint-12000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-12000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in tmp_trainer/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-12000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-12500\n",
      "Configuration saved in tmp_trainer/checkpoint-12500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-12500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-13000\n",
      "Configuration saved in tmp_trainer/checkpoint-13000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-13000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-13500\n",
      "Configuration saved in tmp_trainer/checkpoint-13500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-13500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-14000\n",
      "Configuration saved in tmp_trainer/checkpoint-14000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-14000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-14500\n",
      "Configuration saved in tmp_trainer/checkpoint-14500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-14500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-15000\n",
      "Configuration saved in tmp_trainer/checkpoint-15000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-15000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-15500\n",
      "Configuration saved in tmp_trainer/checkpoint-15500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-15500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-16000\n",
      "Configuration saved in tmp_trainer/checkpoint-16000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-16000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-16500\n",
      "Configuration saved in tmp_trainer/checkpoint-16500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-16500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-17000\n",
      "Configuration saved in tmp_trainer/checkpoint-17000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-17000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-17500\n",
      "Configuration saved in tmp_trainer/checkpoint-17500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-17500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-18000\n",
      "Configuration saved in tmp_trainer/checkpoint-18000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-18000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-18500\n",
      "Configuration saved in tmp_trainer/checkpoint-18500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-18500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-19000\n",
      "Configuration saved in tmp_trainer/checkpoint-19000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-19000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-19500\n",
      "Configuration saved in tmp_trainer/checkpoint-19500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-19500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-20000\n",
      "Configuration saved in tmp_trainer/checkpoint-20000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-20000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-20500\n",
      "Configuration saved in tmp_trainer/checkpoint-20500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-20500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-21000\n",
      "Configuration saved in tmp_trainer/checkpoint-21000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-21000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-21500\n",
      "Configuration saved in tmp_trainer/checkpoint-21500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-21500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-22000\n",
      "Configuration saved in tmp_trainer/checkpoint-22000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-22000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-22500\n",
      "Configuration saved in tmp_trainer/checkpoint-22500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-22500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-23000\n",
      "Configuration saved in tmp_trainer/checkpoint-23000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-23000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-23500\n",
      "Configuration saved in tmp_trainer/checkpoint-23500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-23500/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in tmp_trainer/checkpoint-23500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-24000\n",
      "Configuration saved in tmp_trainer/checkpoint-24000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-24000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-24500\n",
      "Configuration saved in tmp_trainer/checkpoint-24500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-24500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-25000\n",
      "Configuration saved in tmp_trainer/checkpoint-25000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-25000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-25500\n",
      "Configuration saved in tmp_trainer/checkpoint-25500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-25500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-25500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-26000\n",
      "Configuration saved in tmp_trainer/checkpoint-26000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-26000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-26500\n",
      "Configuration saved in tmp_trainer/checkpoint-26500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-26500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-26500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-27000\n",
      "Configuration saved in tmp_trainer/checkpoint-27000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-27000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-27500\n",
      "Configuration saved in tmp_trainer/checkpoint-27500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-27500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-28000\n",
      "Configuration saved in tmp_trainer/checkpoint-28000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-28000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-28500\n",
      "Configuration saved in tmp_trainer/checkpoint-28500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-28500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-29000\n",
      "Configuration saved in tmp_trainer/checkpoint-29000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-29000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-29500\n",
      "Configuration saved in tmp_trainer/checkpoint-29500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-29500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-30000\n",
      "Configuration saved in tmp_trainer/checkpoint-30000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-30000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-30500\n",
      "Configuration saved in tmp_trainer/checkpoint-30500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-30500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-31000\n",
      "Configuration saved in tmp_trainer/checkpoint-31000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-31000/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-31500\n",
      "Configuration saved in tmp_trainer/checkpoint-31500/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-31500/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-31500/special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer/checkpoint-32000\n",
      "Configuration saved in tmp_trainer/checkpoint-32000/config.json\n",
      "Model weights saved in tmp_trainer/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer/checkpoint-32000/special_tokens_map.json\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:274] . unexpected pos 74166976 vs 74166864",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.6.13/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.13/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mbuf_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.13/lib/python3.6/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.13/lib/python3.6/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.13/lib/python3.6/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m             \u001b[0;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1539\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"optimizer.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1540\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcaught_warnings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"scheduler.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.13/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.13/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:274] . unexpected pos 74166976 vs 74166864"
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "761c9ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9823248386383057, 'start': 11, 'end': 16, 'answer': 'black'}\n"
     ]
    }
   ],
   "source": [
    "pred = mp(question=\"Which color is the dog?\", context=\"There is a black dog.\", truncation=True, )\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd68e298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.995335042476654, 'start': 24, 'end': 28, 'answer': 'John'}\n"
     ]
    }
   ],
   "source": [
    "pred = mp(question='Who is the most awesome?', context='William is awesome, but John is more awesome', truncation=True, )\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cbb7072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.8586492538452148, 'start': 48, 'end': 53, 'answer': 'snake'}\n"
     ]
    }
   ],
   "source": [
    "pred = mp(question=\"Which thing is hot?\", context=\"There is a cold gopher, a polar bear, and a hot snake.\", truncation=True, )\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32962747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.4307215213775635, 'start': 11, 'end': 22, 'answer': 'cold gopher'}\n"
     ]
    }
   ],
   "source": [
    "pred = mp(question=\"Which thing is least hot?\", context=\"There is a cold gopher, a polar bear, and a hot snake.\", truncation=True, )\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f589e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bf0e79e",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf46d52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./trained_on_expanded_data_model/\n",
      "Configuration saved in ./trained_on_expanded_data_model/config.json\n",
      "Model weights saved in ./trained_on_expanded_data_model/pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_on_expanded_data_model/tokenizer_config.json\n",
      "Special tokens file saved in ./trained_on_expanded_data_model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('./trained_on_expanded_data_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25e6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (a5)",
   "language": "python",
   "name": "pycharm-f95a7fa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
